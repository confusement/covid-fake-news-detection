{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "from sklearn.metrics import *\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_Eval(Y_Actual,Y_pred):\n",
    "\n",
    "    TrueNegative=0\n",
    "    TruePositive=0\n",
    "    FalsePositive=0\n",
    "    FalseNegative=0\n",
    "    for i in range(len(Y_pred)):\n",
    "        #if the actual class is negative\n",
    "        if(Y_Actual[i]==\"fake\"):\n",
    "            \n",
    "            #if the predicited class is negative\n",
    "            if(Y_pred[i]==\"fake\"):\n",
    "                TrueNegative+=1\n",
    "                \n",
    "            #if the predicted class is positive\n",
    "            else:\n",
    "                FalsePositive+=1\n",
    "                \n",
    "        #if the actual class is positive\n",
    "        else:\n",
    "            #if the predicited class is positive\n",
    "            if(Y_pred[i]==\"real\"):\n",
    "                TruePositive+=1\n",
    "                \n",
    "            #if the predicited class is negative\n",
    "            else:\n",
    "                FalseNegative+=1\n",
    "\n",
    "    Confusion_Matrix=[[TrueNegative,FalsePositive],[FalseNegative,TruePositive]]\n",
    "    \n",
    "    Confusion_Matrix=pd.DataFrame(Confusion_Matrix,columns=['Actual Negative','Actual Positive'])\n",
    "    \n",
    "    Confusion_Matrix.rename(index={0: \"Predicted Negative\", 1: \"Predicted Positive\"},inplace=True)\n",
    "    \n",
    "    MyPrecision=TruePositive/(TruePositive+FalsePositive)\n",
    "\n",
    "    MyRecall=TruePositive/(TruePositive+FalseNegative)\n",
    "    \n",
    "    MyAccuracy=(TruePositive+TrueNegative)/(TruePositive+TrueNegative+FalseNegative+FalsePositive)\n",
    "    \n",
    "    MyF1score= 2*(MyPrecision*MyRecall)/(MyPrecision+MyRecall)\n",
    "    \n",
    "    MyPrecisionZero=TrueNegative/(TrueNegative+FalseNegative)\n",
    "\n",
    "    MyRecallZero=TrueNegative/(TrueNegative+FalsePositive)\n",
    "    \n",
    "    \n",
    "    MyF1scoreZero= 2*(MyPrecisionZero*MyRecallZero)/(MyPrecisionZero+MyRecallZero)\n",
    "\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "    print(\"Accuracy:\",MyAccuracy)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Positive CLass, Label:4\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecision)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecall)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1score)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Negative CLass, Label:0\")\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecisionZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecallZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1scoreZero)\n",
    "\n",
    "    print()\n",
    "    print(\"--------\")\n",
    "    print(\"Macro Average F1\",(MyF1score+MyF1scoreZero)/2)\n",
    "    print(\"-----------------\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(Confusion_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_caller(Tweets_List):\n",
    "    stop_words_list=stopwords.words('english')\n",
    "    for i in (range(len(Tweets_List))):\n",
    "        Tweets_List[i]=preprocess_text(Tweets_List[i],stop_words_list=stop_words_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tweet, flag_stemm=True, flag_lemm=True, stop_words_list=None):\n",
    "#     print(tweet)\n",
    "    tweet=emoji.demojize(tweet)\n",
    "    tweet=tweet.replace(\":\",\" \")\n",
    "#     print(tweet)\n",
    "#     tweet=p.clean(tweet)\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip and remove url)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', str(tweet).lower().strip())\n",
    "#     tweet = re.sub(r'19', 'Z', str(tweet).lower().strip())\n",
    "#     tweet = re.sub(r'[0-9]+k*', ' X ', str(tweet).lower().strip())    \n",
    "#     tweet = re.sub(r'[^\\w\\s]', '', str(tweet).lower().strip())\n",
    "    ## Tokenize (convert from string to list) and remove the stop words\n",
    "#     print(tweet)\n",
    "\n",
    "    tokenize_tweet = tweet.split()\n",
    "\n",
    "    if stop_words_list is not None:\n",
    "        tokenize_tweet = [word for word in tokenize_tweet if word not in stop_words_list]\n",
    "\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flag_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        tokenize_tweet = [ps.stem(word) for word in tokenize_tweet]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flag_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        tokenize_tweet = [lem.lemmatize(word) for word in tokenize_tweet]\n",
    "#     print(tokenize_tweet)\n",
    "    ## back to string from list\n",
    "    tweet = \" \".join(tokenize_tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=pd.DataFrame(pd.read_csv(\"Constraint_English_Train - Sheet1.csv\"))\n",
    "df_training.drop(columns=\"id\",inplace=True)\n",
    "# df_training\n",
    "df_training['tweet_original']=df_training['tweet']\n",
    "preprocess_text_caller(df_training['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation=pd.DataFrame(pd.read_csv(\"Constraint_English_Val - Sheet1.csv\"))\n",
    "df_validation.drop(columns=\"id\",inplace=True)\n",
    "# df_validation\n",
    "df_validation['tweet_original']=df_validation['tweet']\n",
    "preprocess_text_caller(df_validation['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-fb9ae317fd00>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Tweets_List[i]=preprocess_text(Tweets_List[i],stop_words_list=stop_words_list)\n"
     ]
    }
   ],
   "source": [
    "df_test=pd.DataFrame(pd.read_csv(\"Constraint_English_Test - Sheet1.csv\"))\n",
    "# df_test\n",
    "df_test['tweet_original']=df_test['tweet']\n",
    "preprocess_text_caller(df_test['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_ngrams=vectorizer.fit_transform(df_training.tweet)\n",
    "vectorizer.fit_transform(list(df_training.tweet) + list(df_validation.tweet) +list(df_test.tweet))\n",
    "Train_ngrams=vectorizer.transform(df_training.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_ngrams=vectorizer.transform(df_validation.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2140x28361 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50472 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Validation_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_ngrams=vectorizer.transform(df_test.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9214293769160768"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_rf=rf.predict(Validation_ngrams)\n",
    "f1_score(y_pred_rf,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420009074925841"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svc = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\"))\n",
    "svc.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_svm=svc.predict(Validation_ngrams)\n",
    "f1_score(y_pred_svm,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9199220797238565"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression(random_state=0)\n",
    "log.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_log=log.predict(Validation_ngrams)\n",
    "f1_score(y_pred_log,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93259630224574"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(random_state=1, max_iter=300,verbose=True,activation=\"tanh\",solver=\"lbfgs\")\n",
    "mlp.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_mlp=mlp.predict(Validation_ngrams)\n",
    "f1_score(y_pred_mlp,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9480886623094711"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "svc1= make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\"))\n",
    "bag = BaggingClassifier(svc1,n_estimators=100,random_state=0,n_jobs=-1)\n",
    "bag.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_bag=bag.predict(Validation_ngrams)\n",
    "f1_score(y_pred_bag,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in range(len(y_pred_svm)):\n",
    "    if(y_pred_svm[i]==y_pred_bag[i]):\n",
    "        y.append(y_pred_bag[i])\n",
    "    elif(y_pred_bag[i]==y_pred_mlp[i]):\n",
    "        y.append(y_pred_bag[i])\n",
    "    elif(y_pred_svm[i]==y_pred_mlp[i]):\n",
    "        y.append(y_pred_mlp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9480829440209819"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_Eval(y,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.9481308411214954\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.9330357142857143\n",
      "-----------------\n",
      "Recall: 0.9666975023126735\n",
      "-----------------\n",
      "F1 Score: 0.9495683780099955\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.9647058823529412\n",
      "-----------------\n",
      "Recall: 0.9291784702549575\n",
      "-----------------\n",
      "F1 Score: 0.9466089466089466\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.9480886623094711\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                    Actual Negative  Actual Positive\n",
      "Predicted Negative              984               75\n",
      "Predicted Positive               36             1045\n"
     ]
    }
   ],
   "source": [
    "func_Eval(y_pred_bag,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9480886623094711\n"
     ]
    }
   ],
   "source": [
    "y_pred=bag.predict(Validation_ngrams)\n",
    "print(f1_score(df_validation[\"label\"],y_pred,average=\"macro\"))\n",
    "y_pred_test=bag.predict(Test_ngrams)\n",
    "y_pred_test=pd.DataFrame(y_pred_test)\n",
    "ans=pd.DataFrame(df_test[\"id\"])\n",
    "ans[\"label\"]=y_pred_test\n",
    "ans.to_csv(\"answer_bagging_9488.txt\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open(\"websites_test\",\"rb\")\n",
    "websites_test=pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file=open(\"websites_train\",\"rb\")\n",
    "websites_train=pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file=open(\"websites_valid\",\"rb\")\n",
    "websites_valid=pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4314/4314 [00:00<00:00, 686661.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# len(websites)\n",
    "from requests.exceptions import SSLError,ConnectionError\n",
    "for i in tqdm(range(len(websites_train))):\n",
    "    if(\"t.co\" in websites_train[i][1]):\n",
    "        try:\n",
    "            websites_train[i][1]=requests.get(websites_train[i][1]).url\n",
    "        except SSLError:\n",
    "            print(\"error\")\n",
    "            continue\n",
    "        except ConnectionError:\n",
    "            continue\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open(\"websites_train\",\"wb\")\n",
    "pickle.dump(websites_train,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1459/1459 [00:00<00:00, 1116898.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# len(websites)\n",
    "from requests.exceptions import SSLError\n",
    "for i in tqdm(range(len(websites_valid))):\n",
    "    if(\"t.co\" in websites_valid[i][1]):\n",
    "        try:\n",
    "            websites_valid[i][1]=requests.get(websites_valid[i][1]).url\n",
    "        except SSLError:\n",
    "            print(\"error\")\n",
    "            continue\n",
    "        except :\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open(\"websites_valid\",\"wb\")\n",
    "pickle.dump(websites_valid,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [00:00<00:00, 460269.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from requests.exceptions import SSLError\n",
    "for i in tqdm(range(len(websites_test))):\n",
    "    if(\"t.co\" in websites_test[i][1]):\n",
    "\n",
    "        try:\n",
    "            websites_test[i][1]=requests.get(websites_test[i][1]).url\n",
    "        except SSLError:\n",
    "            print(\"error\")\n",
    "            continue\n",
    "        except :\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open(\"websites_test\",\"wb\")\n",
    "pickle.dump(websites_test,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "web=set()\n",
    "for i in websites_train:\n",
    "    if(\"t.co\" not in i[1]):\n",
    "        web.add(i[1][:min(20,len(i[1]))].replace(\"http://\",\"\").replace(\"https://\",\"\").replace(\"www.\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in websites_valid:\n",
    "    if(\"t.co\" not in i[1]):\n",
    "        web.add(i[1][:min(20,len(i[1]))].replace(\"http://\",\"\").replace(\"https://\",\"\").replace(\"www.\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(web2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in websites_test:\n",
    "    if(\"t.co\" not in i[1]):\n",
    "        web.add(i[1][:min(20,len(i[1]))].replace(\"http://\",\"\").replace(\"https://\",\"\").replace(\"www.\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1011now.',\n",
       " '11alive.',\n",
       " 'aapf.org/aap',\n",
       " 'abc.net.',\n",
       " 'abc.us3.list',\n",
       " 'accounts.goo',\n",
       " 'acpjourn',\n",
       " 'africacdc.or',\n",
       " 'ageofaut',\n",
       " 'aljazeer',\n",
       " 'amp.cnn.com/',\n",
       " 'amp.theatlan',\n",
       " 'amp.wbur.org',\n",
       " 'aninews.',\n",
       " 'arogya.mahar',\n",
       " 'assets.ctfas',\n",
       " 'axios.co',\n",
       " 'azdhs.go',\n",
       " 'bbc.co.u',\n",
       " 'bendigoa',\n",
       " 'bethe1to',\n",
       " 'betootaa',\n",
       " 'blog.covidac',\n",
       " 'blog.rootcla',\n",
       " 'bloomber',\n",
       " 'bmjopen.bmj.',\n",
       " 'brighteo',\n",
       " 'bu.edu/a',\n",
       " 'business',\n",
       " 'businessinth',\n",
       " 'cbc.ca/n',\n",
       " 'cbsnews.',\n",
       " 'ccc19.org/',\n",
       " 'cdc.gov/',\n",
       " 'cddwesta',\n",
       " 'cdph.ca.',\n",
       " 'census.g',\n",
       " 'civilavi',\n",
       " 'cla.purd',\n",
       " 'clinical',\n",
       " 'clinowl.com/',\n",
       " 'cms.gov/',\n",
       " 'cnbc.com',\n",
       " 'confirmsubsc',\n",
       " 'consultqd.cl',\n",
       " 'copcov.o',\n",
       " 'coronavirus-',\n",
       " 'coronavirus.',\n",
       " 'covid.cdc.go',\n",
       " 'covid19.govt',\n",
       " 'covid19.ncdc',\n",
       " 'covid19.tela',\n",
       " 'covid19dashbo',\n",
       " 'covid19respo',\n",
       " 'covidactnow.',\n",
       " 'covidly.com/',\n",
       " 'covidtrackin',\n",
       " 'crooked.com/',\n",
       " 'customs.',\n",
       " 'd4bl.org/',\n",
       " 'd4bl.org/rep',\n",
       " 'dailybusines',\n",
       " 'dailyinterla',\n",
       " 'dailyite',\n",
       " 'dailymai',\n",
       " 'docs.google.',\n",
       " 'doh.wa.g',\n",
       " 'dreddymd.com',\n",
       " 'duffelbl',\n",
       " 'eastvall',\n",
       " 'ed.ac.uk',\n",
       " 'edition.cnn.',\n",
       " 'emedicine.me',\n",
       " 'emergency.cd',\n",
       " 'en.m.wikiped',\n",
       " 'epochtim',\n",
       " 'erj.ersjourn',\n",
       " 'eventbri',\n",
       " 'experience.a',\n",
       " 'facebook',\n",
       " 'factchec',\n",
       " 'floridad',\n",
       " 'floridat',\n",
       " 'forbettersci',\n",
       " 'fox5atla',\n",
       " 'foxnews.',\n",
       " 'ftadvise',\n",
       " 'getusppe.org',\n",
       " 'ghpp.de/en/p',\n",
       " 'gisgmda.maps',\n",
       " 'github.com/C',\n",
       " 'google.c',\n",
       " 'gossipco',\n",
       " 'gujcovid19.g',\n",
       " 'health.g',\n",
       " 'health.odish',\n",
       " 'hhmi.org',\n",
       " 'hindusta',\n",
       " 'hmfw.ap.gov.i',\n",
       " 'hoops227.biz',\n",
       " 'hoops227.sho',\n",
       " 'houstonc',\n",
       " 'https:/…',\n",
       " 'https:…',\n",
       " 'https…',\n",
       " 'http…',\n",
       " 'icmr.gov',\n",
       " 'ilcuk.org.uk',\n",
       " 'imperial',\n",
       " 'indiatvn',\n",
       " 'infogram.com',\n",
       " 'instagra',\n",
       " 'interloc',\n",
       " 'investors.mo',\n",
       " 'j-idea.githu',\n",
       " 'jaad.org',\n",
       " 'joebiden.com',\n",
       " 'journalo',\n",
       " 'journals.lww',\n",
       " 'khn.org/news',\n",
       " 'latimes.',\n",
       " 'local.nihr.a',\n",
       " 'm.timesofind',\n",
       " 'mailchi.mp/a',\n",
       " 'mailchi.mp/p',\n",
       " 'mdedge.c',\n",
       " 'mediabiasfac',\n",
       " 'medium.com/@',\n",
       " 'medpaget',\n",
       " 'medrxiv.',\n",
       " 'medscape',\n",
       " 'mha.gov.',\n",
       " 'mixlr.com/hi',\n",
       " 'modernat',\n",
       " 'mohfw.go',\n",
       " 'msn.com/e',\n",
       " 'nature.c',\n",
       " 'nbcnews.',\n",
       " 'ncdc.gov.ng/',\n",
       " 'nejm.org',\n",
       " 'newindia',\n",
       " 'news.gallup.',\n",
       " 'news.sky.com',\n",
       " 'news.yahoo.c',\n",
       " 'newschecker.',\n",
       " 'newsfactsnet',\n",
       " 'newsfromwale',\n",
       " 'newsthump.co',\n",
       " 'newsweek',\n",
       " 'nih.gov/',\n",
       " 'nitp.ncdc.go',\n",
       " 'nnn.ng/natio',\n",
       " 'north-wales-',\n",
       " 'npr.org/',\n",
       " 'nytimes.',\n",
       " 'okinawa.stri',\n",
       " 'omaha.com/li',\n",
       " 'onlinelibrar',\n",
       " 'oregonli',\n",
       " 'outlooki',\n",
       " 'pandemic',\n",
       " 'patents.just',\n",
       " 'pennhills.or',\n",
       " 'perchcow',\n",
       " 'pharmanews.l',\n",
       " 'pib.gov.',\n",
       " 'pib.gov.in/P',\n",
       " 'pinalcen',\n",
       " 'pnas.org',\n",
       " 'politico',\n",
       " 'portal.ct.go',\n",
       " 'poynter.',\n",
       " 'pscp.tv/',\n",
       " 'public.table',\n",
       " 'publiche',\n",
       " 'pubmed.ncbi.',\n",
       " 'punchng.com/',\n",
       " 'qns.com/stor',\n",
       " 'recovery',\n",
       " 'reference.me',\n",
       " 'revenue.delhi',\n",
       " 'riverheadloc',\n",
       " 'rockefel',\n",
       " 'science.scie',\n",
       " 'sciencejourn',\n",
       " 'scmp.com',\n",
       " 'scotch-w',\n",
       " 'sfchroni',\n",
       " 'shop.newsthu',\n",
       " 'shorturl',\n",
       " 'sltrib.c',\n",
       " 'smebusinessn',\n",
       " 'socoemergenc',\n",
       " 'sputniknews.',\n",
       " 'standard',\n",
       " 'statnews',\n",
       " 'styleupnow.c',\n",
       " 't.c…',\n",
       " 'theatlan',\n",
       " 'theaustr',\n",
       " 'thebeave',\n",
       " 'thebeaverton',\n",
       " 'thedaily',\n",
       " 'theeagleonli',\n",
       " 'theflashtoda',\n",
       " 'theguard',\n",
       " 'thehill.com/',\n",
       " 'thespoof',\n",
       " 'theweek.com/',\n",
       " 'thip.med',\n",
       " 'thisisreno.c',\n",
       " 'time.com/581',\n",
       " 'time.com/583',\n",
       " 'timesofindia',\n",
       " 'transpor',\n",
       " 'twitter.com/',\n",
       " 'txdshs.maps.',\n",
       " 'uk-business-',\n",
       " 'us02web.zoom',\n",
       " 'usatoday',\n",
       " 'vanityfa',\n",
       " 'vice.com',\n",
       " 'vox.com/',\n",
       " 'wacotrib.com',\n",
       " 'wandtv.c',\n",
       " 'waterfordwhi',\n",
       " 'watsupameric',\n",
       " 'wbhealth',\n",
       " 'wdet.org/pos',\n",
       " 'who.int/',\n",
       " 'windy.co',\n",
       " 'wionews.',\n",
       " 'wnd.com/',\n",
       " 'worldome',\n",
       " 'wqth.wordpre',\n",
       " 'wsj.com/',\n",
       " 'www1.nyc.gov',\n",
       " 'wwwnc.cdc.go',\n",
       " 'youtube.',\n",
       " 'zeenews.indi'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={}\n",
    "j=0\n",
    "for i in web:\n",
    "    dic[i]=j\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_web=[]\n",
    "tfidf_train_web=[]\n",
    "for i in range(len(df_training)):\n",
    "    temp=[]\n",
    "    temp2=\"\"\n",
    "    for j in range(len(web)):\n",
    "        temp.append(0)\n",
    "    train_web.append(temp)\n",
    "    tfidf_train_web.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for pair in websites_train:\n",
    "    index=pair[0]\n",
    "    website=pair[1]\n",
    "    for i in web:\n",
    "        if(i in website):\n",
    "            count+=1\n",
    "#             print(i)\n",
    "            train_web[index][dic[i]]+=1\n",
    "            tfidf_train_web[index]+=i+\" \"\n",
    "#             print(train_web[index])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=train_web\n",
    "train_web=pd.DataFrame(train_web)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_web=[]\n",
    "tfidf_valid_web=[]\n",
    "for i in range(len(df_validation)):\n",
    "    temp=[]\n",
    "    temp2=\"\"\n",
    "    for j in range(len(web)):\n",
    "        temp.append(0)\n",
    "    valid_web.append(temp)\n",
    "    tfidf_valid_web.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for pair in websites_valid:\n",
    "    index=pair[0]\n",
    "    website=pair[1]\n",
    "    for i in web:\n",
    "        if(i in website):\n",
    "            count+=1\n",
    "#             print(i)\n",
    "            valid_web[index][dic[i]]+=1\n",
    "            tfidf_valid_web[index]+=i+\" \"\n",
    "#             print(valid_web[index])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs+=valid_web\n",
    "valid_web=pd.DataFrame(valid_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_web=[]\n",
    "for i in range(len(df_test)):\n",
    "    temp=[]\n",
    "    temp2=\"\"\n",
    "    for j in range(len(web)):\n",
    "        temp.append(0)\n",
    "    test_web.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for pair in websites_test:\n",
    "    index=pair[0]\n",
    "    website=pair[1]\n",
    "    for i in web:\n",
    "        if(i in website):\n",
    "            count+=1\n",
    "#             print(i)\n",
    "            test_web[index][dic[i]]+=1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs+=test_web\n",
    "test_web=pd.DataFrame(test_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# csr_TrainFeature_without_ngram=csr_matrix(train_web)\n",
    "# csr_ValidFeature_without_ngram=csr_matrix(valid_web)\n",
    "# TrainFeatureFinal=hstack([Train_ngrams,csr_TrainFeature_without_ngram])\n",
    "# ValidFeatureFinal=hstack([Validation_ngrams,csr_ValidFeature_without_ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\"))\n",
    "# clf.fit(TrainFeatureFinal,df_training[\"label\"])\n",
    "# y_pred=clf.predict(ValidFeatureFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(y_pred,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# svc1= make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\"))\n",
    "# bag = BaggingClassifier(svc1,n_estimators=100,random_state=0,n_jobs=-1)\n",
    "# bag.fit(TrainFeatureFinal,df_training[\"label\"])\n",
    "# y_pred_bag=bag.predict(ValidFeatureFinal)\n",
    "# f1_score(y_pred_bag,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(docs)\n",
    "train = tfidf.transform(train_web)\n",
    "valid = tfidf.transform(valid_web)\n",
    "test  = tfidf.transform(test_web) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_TrainFeatureFinal=hstack([Train_ngrams,train])\n",
    "tfidf_ValidFeatureFinal=hstack([Validation_ngrams,valid])\n",
    "tfidf_TestFeatureFinal =hstack([Test_ngrams,test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\"))\n",
    "clf.fit(tfidf_TrainFeatureFinal,df_training[\"label\"])\n",
    "y_pred=clf.predict(tfidf_ValidFeatureFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9550805113614996"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9569606262307582"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "svc1= make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\"))\n",
    "bag = BaggingClassifier(svc1,n_estimators=100,random_state=0,n_jobs=-1)\n",
    "bag.fit(tfidf_TrainFeatureFinal,df_training[\"label\"])\n",
    "y_pred_bag=bag.predict(tfidf_ValidFeatureFinal)\n",
    "f1_score(y_pred_bag,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9349130047299994"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression(random_state=0)\n",
    "log.fit(tfidf_TrainFeatureFinal,df_training[\"label\"])\n",
    "y_pred_log=log.predict(tfidf_ValidFeatureFinal)\n",
    "f1_score(y_pred_log,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93259630224574"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(random_state=1, max_iter=300,verbose=True,activation=\"tanh\",solver=\"lbfgs\")\n",
    "mlp.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_mlp=mlp.predict(Validation_ngrams)\n",
    "f1_score(y_pred_mlp,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9541537105501554"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[]\n",
    "for i in range(len(y_pred_svm)):\n",
    "    if(y_pred_svm[i]==y_pred_bag[i]):\n",
    "        y.append(y_pred_bag[i])\n",
    "    elif(y_pred_bag[i]==y_pred_mlp[i]):\n",
    "        y.append(y_pred_bag[i])\n",
    "    elif(y_pred_svm[i]==y_pred_mlp[i]):\n",
    "        y.append(y_pred_mlp[i])\n",
    "f1_score(y,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9569606262307582\n"
     ]
    }
   ],
   "source": [
    "y_pred=bag.predict(tfidf_ValidFeatureFinal)\n",
    "print(f1_score(df_validation[\"label\"],y_pred,average=\"macro\"))\n",
    "y_pred_test=bag.predict(tfidf_TestFeatureFinal)\n",
    "y_pred_test=pd.DataFrame(y_pred_test)\n",
    "ans=pd.DataFrame(df_test[\"id\"])\n",
    "ans[\"label\"]=y_pred_test\n",
    "ans.to_csv(\"answer_9569_valid_web.txt\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
