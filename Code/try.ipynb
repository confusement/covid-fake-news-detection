{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_Eval(Y_Actual,Y_pred):\n",
    "\n",
    "    TrueNegative=0\n",
    "    TruePositive=0\n",
    "    FalsePositive=0\n",
    "    FalseNegative=0\n",
    "    for i in range(len(Y_pred)):\n",
    "        #if the actual class is negative\n",
    "        if(Y_Actual[i]==\"fake\"):\n",
    "            \n",
    "            #if the predicited class is negative\n",
    "            if(Y_pred[i]==\"fake\"):\n",
    "                TrueNegative+=1\n",
    "                \n",
    "            #if the predicted class is positive\n",
    "            else:\n",
    "                FalsePositive+=1\n",
    "                \n",
    "        #if the actual class is positive\n",
    "        else:\n",
    "            #if the predicited class is positive\n",
    "            if(Y_pred[i]==\"real\"):\n",
    "                TruePositive+=1\n",
    "                \n",
    "            #if the predicited class is negative\n",
    "            else:\n",
    "                FalseNegative+=1\n",
    "\n",
    "    Confusion_Matrix=[[TrueNegative,FalsePositive],[FalseNegative,TruePositive]]\n",
    "    \n",
    "    Confusion_Matrix=pd.DataFrame(Confusion_Matrix,columns=['Predicted Negative','Predicted Positive'])\n",
    "    \n",
    "    Confusion_Matrix.rename(index={0: \"Actual Negative\", 1: \"Actual Positive\"},inplace=True)\n",
    "    \n",
    "    MyPrecision=TruePositive/(TruePositive+FalsePositive)\n",
    "\n",
    "    MyRecall=TruePositive/(TruePositive+FalseNegative)\n",
    "    \n",
    "    MyAccuracy=(TruePositive+TrueNegative)/(TruePositive+TrueNegative+FalseNegative+FalsePositive)\n",
    "    \n",
    "    MyF1score= 2*(MyPrecision*MyRecall)/(MyPrecision+MyRecall)\n",
    "    \n",
    "    MyPrecisionZero=TrueNegative/(TrueNegative+FalseNegative)\n",
    "\n",
    "    MyRecallZero=TrueNegative/(TrueNegative+FalsePositive)\n",
    "    \n",
    "    \n",
    "    MyF1scoreZero= 2*(MyPrecisionZero*MyRecallZero)/(MyPrecisionZero+MyRecallZero)\n",
    "\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "    print(\"Accuracy:\",MyAccuracy)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Positive CLass, Label:4\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecision)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecall)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1score)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Negative CLass, Label:0\")\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecisionZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecallZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1scoreZero)\n",
    "\n",
    "    print()\n",
    "    print(\"--------\")\n",
    "    print(\"Macro Average F1\",(MyF1score+MyF1scoreZero)/2)\n",
    "    print(\"-----------------\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(Confusion_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_caller(Tweets_List):\n",
    "    stop_words_list=stopwords.words('english')\n",
    "    for i in (range(len(Tweets_List))):\n",
    "        Tweets_List[i]=preprocess_text(Tweets_List[i],stop_words_list=stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tweet, flag_stemm=True, flag_lemm=False, stop_words_list=None):\n",
    "#     print(tweet)\n",
    "    tweet=emoji.demojize(tweet)\n",
    "    tweet=tweet.replace(\":\",\" \")\n",
    "#     print(tweet)\n",
    "    tweet=p.clean(tweet)\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip and remove url)\n",
    "    \n",
    "    tweet = re.sub(r'[^\\w\\s]', '', str(tweet).lower().strip())\n",
    "    ## Tokenize (convert from string to list) and remove the stop words\n",
    "#     print(tweet)\n",
    "\n",
    "    tokenize_tweet = tweet.split()\n",
    "\n",
    "    if stop_words_list is not None:\n",
    "        tokenize_tweet = [word for word in tokenize_tweet if word not in stop_words_list]\n",
    "\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flag_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        tokenize_tweet = [ps.stem(word) for word in tokenize_tweet]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flag_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        tokenize_tweet = [lem.lemmatize(word) for word in tokenize_tweet]\n",
    "#     print(tokenize_tweet)\n",
    "    ## back to string from list\n",
    "    tweet = \" \".join(tokenize_tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=pd.DataFrame(pd.read_csv(\"Constraint_English_Train - Sheet1.csv\"))\n",
    "df_training.drop(columns=\"id\",inplace=True)\n",
    "df_training\n",
    "df_training['tweet_original']=df_training['tweet']\n",
    "preprocess_text_caller(df_training['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# ngram_range parameter (1,2) means that unigram and bigram will be taken\n",
    "# Count Vecotrizer automatically preprocess the tweets\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4),min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_ngrams=vectorizer.fit_transform(df_training.tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(len(tweets_test))):\n",
    "#     #Removing Urls, mentions, hashtags\n",
    "#     res=p.clean(tweets_test[i])\n",
    "#     res = re.sub(r'[^\\w\\s]', '',res)\n",
    "#     tweets_test[i]=res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation=pd.DataFrame(pd.read_csv(\"Constraint_English_Val - Sheet1.csv\"))\n",
    "df_validation.drop(columns=\"id\",inplace=True)\n",
    "df_validation\n",
    "df_validation['tweet_original']=df_validation['tweet']\n",
    "preprocess_text_caller(df_validation['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_ngrams=vectorizer.transform(df_validation.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2140x5299 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 37215 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Validation_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred=clf.predict(Validation_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9054402274964453"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.905607476635514\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.9053571428571429\n",
      "-----------------\n",
      "Recall: 0.9135135135135135\n",
      "-----------------\n",
      "F1 Score: 0.9094170403587443\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.9058823529411765\n",
      "-----------------\n",
      "Recall: 0.8970873786407767\n",
      "-----------------\n",
      "F1 Score: 0.9014634146341464\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.9054402274964453\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative                 924                 106\n",
      "Actual Positive                  96                1014\n"
     ]
    }
   ],
   "source": [
    "f1_score(y_pred,df_validation[\"label\"],average=\"macro\")\n",
    "func_Eval(y_pred,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\",solver=\"\"))\n",
    "clf.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred=clf.predict(Validation_ngrams)\n",
    "f1_score(y_pred,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9246530121846757"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.9247663551401869\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.9205357142857142\n",
      "-----------------\n",
      "Recall: 0.9347234814143246\n",
      "-----------------\n",
      "F1 Score: 0.9275753486279802\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.9294117647058824\n",
      "-----------------\n",
      "Recall: 0.914175506268081\n",
      "-----------------\n",
      "F1 Score: 0.921730675741371\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.9246530121846757\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative                 948                  89\n",
      "Actual Positive                  72                1031\n"
     ]
    }
   ],
   "source": [
    "func_Eval(y_pred,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-fb9ae317fd00>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Tweets_List[i]=preprocess_text(Tweets_List[i],stop_words_list=stop_words_list)\n"
     ]
    }
   ],
   "source": [
    "df_test=pd.DataFrame(pd.read_csv(\"Constraint_English_Test - Sheet1.csv\"))\n",
    "df_test\n",
    "df_test['tweet_original']=df_validation['tweet']\n",
    "preprocess_text_caller(df_test['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_ngrams=vectorizer.transform(df_validation.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9246530121846757"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(Validation_ngrams)\n",
    "f1_score(df_validation[\"label\"],y_pred,average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test=clf.predict(Test_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test=pd.DataFrame(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=pd.DataFrame(df_test[\"id\"])\n",
    "ans[\"label\"]=y_pred_test\n",
    "ans.to_csv(\"answer.txt\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf2 = MLPClassifier(random_state=1, max_iter=300,verbose=True,activation=\"logistic\",solver=\"lbfgs\")\n",
    "clf2.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred=clf2.predict(Validation_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9189328442414445"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.9191588785046729\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.9285714285714286\n",
      "-----------------\n",
      "Recall: 0.9179170344218888\n",
      "-----------------\n",
      "F1 Score: 0.9232134931202841\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.9088235294117647\n",
      "-----------------\n",
      "Recall: 0.9205561072492552\n",
      "-----------------\n",
      "F1 Score: 0.9146521953626049\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.9189328442414445\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative                 927                  80\n",
      "Actual Positive                  93                1040\n"
     ]
    }
   ],
   "source": [
    "func_Eval(y_pred,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pysbd.utils import PySBDFactory\n",
    "\n",
    "import urllib\n",
    "import requests, re, spacy\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import csv\n",
    "import preprocessor as p\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "ua = UserAgent()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# f = open('CommonWords.txt')\n",
    "# commonEngWords = f.read().splitlines()\n",
    "# f.close()\n",
    "popular_links = [\n",
    "        \"nytimes\", \"wsj\", \"huffpost\", \"washingtonpost\",\"time\",\"republicworld\",\n",
    "        \"latimes\", \"reuters\", \"abcnews\", \"usatoday\",\n",
    "        \"bloomberg\", \"nbcnews\", \"dailymail\", \"theguardian\",\n",
    "        \"thesun\", \"mirror\", \"telegraph\", \"bbc\",\n",
    "        \"thestar\", \"theglobeandmail\", \"forbes\",\n",
    "        \"cnbc\", \"chinadaily\", \"nypost\", \"usnews\",\n",
    "        \"timesofindia\", \"thehindu\", \"hindustantimes\",\n",
    "        \"cbsnews\", \"sfgate\", \"thehill\", \"thedailybeast\",\n",
    "        \"newsweek\", \"theatlantic\", \"nzherald\", \"vanguardngr\",\n",
    "        \"dailysun\", \"thejakartapost\", \"thestar\",\n",
    "        \"straitstimes\", \"bangkokpost\", \"japantimes\",\n",
    "        \"thedailystar\", \"scmp\", \"yahoo.com/news\", \"news.google\"\n",
    "        ]\n",
    "\n",
    "\n",
    "def levenshtein_ratio_and_distance(s, t, ratio_calc = False):\n",
    "    \"\"\" levenshtein_ratio_and_distance:\n",
    "        Calculates levenshtein distance between two strings.\n",
    "        If ratio_calc = True, the function computes the\n",
    "        levenshtein distance ratio of similarity between two strings\n",
    "        For all i and j, distance[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    distance = np.zeros((rows,cols),dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1,cols):\n",
    "            distance[i][0] = i\n",
    "            distance[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
    "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
    "                if ratio_calc == True:\n",
    "                    cost = 2\n",
    "                else:\n",
    "                    cost = 1\n",
    "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
    "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
    "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
    "    if ratio_calc == True:\n",
    "        # Computation of the Levenshtein Distance Ratio\n",
    "        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))\n",
    "        return Ratio\n",
    "    else:\n",
    "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
    "        # insertions and/or substitutions\n",
    "        # This is the minimum number of edits needed to convert string a to string b\n",
    "        return \"The strings are {} edits away\".format(distance[row][col])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_search_result(query, number_result = 20):\n",
    "    query = urllib.parse.quote_plus(query) # Format into URL encoding\n",
    "    \n",
    "    \n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(number_result)\n",
    "    response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "    \n",
    "    links = []\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        # Checks if each element is present, else, raise exception\n",
    "        try:\n",
    "            link = r.find('a', href = True)\n",
    "            title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "            description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "            \n",
    "            # Check to make sure everything is present before appending\n",
    "            if link != '' and title != '' and description != '': \n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "        # Next loop if one element is not present\n",
    "        except:\n",
    "            continue\n",
    "    return links, titles, descriptions\n",
    "\n",
    " \n",
    "def clean_links(links, titles, descriptions):\n",
    "    to_remove = []\n",
    "    clean_links = []\n",
    "    for i, l in enumerate(links):\n",
    "        clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
    "    \n",
    "        # Anything that doesn't fit the above pattern will be removed\n",
    "        if clean is None:\n",
    "            to_remove.append(i)\n",
    "            continue\n",
    "        clean_links.append(clean.group(1))\n",
    "    \n",
    "    # Remove the corresponding titles & descriptions\n",
    "    for x in to_remove:\n",
    "        del titles[x]\n",
    "        del descriptions[x]\n",
    "    return clean_links, titles, descriptions\n",
    "\n",
    "\n",
    "def filter_links(links, titles, descriptions):\n",
    "    to_remove = []\n",
    "    for i, l in enumerate(links):\n",
    "        if not any(a in l for a in popular_links):\n",
    "            to_remove.append(i)\n",
    "            continue\n",
    "    \n",
    "    # Remove the corresponding titles & descriptions\n",
    "    links = [l for i,l in enumerate(links) if i not in to_remove]\n",
    "    titles = [t for i,t in enumerate(titles) if i not in to_remove]\n",
    "    links = [l for i,d in enumerate(descriptions) if i not in to_remove]\n",
    "    return links, titles, descriptions\n",
    "    \n",
    "\n",
    "def valid_links(links):\n",
    "    no_link = 0\n",
    "    for link in links:\n",
    "        if any(implink in link for implink in popular_links):\n",
    "            no_link = no_link+1\n",
    "    return no_link/len(links)\n",
    "\n",
    "\n",
    "def split_sentence(text):\n",
    "    nlp = spacy.blank('en')\n",
    "    nlp.add_pipe(PySBDFactory(nlp))\n",
    "    doc = nlp(text)\n",
    "    return([sent.text for sent in doc.sents if sent.text.isspace()==False])\n",
    "\n",
    "\n",
    "def valid_description(query, descriptions):\n",
    "    temp_query = preprocessing(query)\n",
    "    sentences = split_sentence(temp_query)\n",
    "    score = []\n",
    "    for q in query:\n",
    "        temp = [levenshtein_ratio_and_distance(q, sent,ratio_calc = True) for sent in sentences]\n",
    "        score.append(max(temp))\n",
    "    return sum(score)/len(score)\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "    text = p.clean(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = text.lower().replace('[^\\w\\s]',' ').replace('\\s\\s+', ' ').replace('@','').replace('#','. ').replace('&amp;', 'and')\n",
    "    return text\n",
    "\n",
    "\n",
    "def translate_text(text):\n",
    "    result = translator.translate('Mitä sinä teet')\n",
    "    return result.text\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2140/2140 [27:44<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "score=[]\n",
    "for query in tqdm(df_validation.tweet):\n",
    "    links, titles, descriptions = get_search_result(query)\n",
    "    links, titles, descriptions = filter_links(links, titles, descriptions)\n",
    "    #descriptions = [preprocessing(desc) for desc in descriptions]\n",
    "    if len(titles)>0:\n",
    "        titles = [preprocessing(title.lower()) for title in titles]\n",
    "        desc_score = valid_description(query, titles)\n",
    "        score.append(desc_score)\n",
    "    else:\n",
    "        score.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis=score2+score+score3\n",
    "# len(score3)+len(score2)+len(score)\n",
    "# len(lis)\n",
    "# len(score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open(\"score_validaiton\",'wb')\n",
    "pickle.dump(score,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2140"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=open(\"score_validaiton\",'rb')\n",
    "s=pickle.load(file)\n",
    "file.close()\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lis=[]\n",
    "for i in lis:\n",
    "        train_lis.append([i])\n",
    "valid_lis=[]\n",
    "for i in score:\n",
    "        valid_lis.append([i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "csr_TrainFeature_without_ngram=csr_matrix(train_lis)\n",
    "csr_ValidFeature_without_ngram=csr_matrix(valid_lis)\n",
    "TrainFeatureFinal=hstack([Train_ngrams,csr_TrainFeature_without_ngram])\n",
    "ValidFeatureFinal=hstack([Validation_ngrams,csr_ValidFeature_without_ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\"))\n",
    "clf.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred=clf.predict(Validation_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9246530121846757"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.9247663551401869\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.9205357142857142\n",
      "-----------------\n",
      "Recall: 0.9347234814143246\n",
      "-----------------\n",
      "F1 Score: 0.9275753486279802\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.9294117647058824\n",
      "-----------------\n",
      "Recall: 0.914175506268081\n",
      "-----------------\n",
      "F1 Score: 0.921730675741371\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.9246530121846757\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative                 948                  89\n",
      "Actual Positive                  72                1031\n"
     ]
    }
   ],
   "source": [
    "func_Eval(y_pred,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
