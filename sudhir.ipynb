{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "#Word2Vec\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "#Navneet\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getF1(confusion):\n",
    "    print(\"Accuracy:\", getAccuracy(confusion))\n",
    "    numClass = confusion.shape[0]\n",
    "    total = np.sum(np.sum(confusion))\n",
    "    \n",
    "    # get True positive from diagonals\n",
    "    tp = confusion.diagonal().reshape((numClass,1)).copy()\n",
    "    \n",
    "    # make diagonal zeros\n",
    "    confusionCopy = confusion.copy()\n",
    "    np.fill_diagonal(confusionCopy,0)\n",
    "    fn = np.sum(confusionCopy,axis=1).reshape(((numClass,1))).copy()\n",
    "    fp = np.sum(confusionCopy,axis=0).reshape(((numClass,1))).copy()\n",
    "    if(np.sum(tp+fp)==0 or np.isfinite(tp+fp).all()==False):\n",
    "        print(tp,fp)\n",
    "        prec = tp\n",
    "    else:\n",
    "        prec = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    f1 = (2*prec*recall) / (prec+recall)\n",
    "    \n",
    "    microPrec = np.sum(tp) / (np.sum(tp)+np.sum(fp))\n",
    "    microRecall = np.sum(tp) / (np.sum(tp)+np.sum(fn))\n",
    "    microf1 = (2*microPrec*microRecall) / (microPrec+microRecall)\n",
    "    \n",
    "    macroPrec = np.mean(prec)\n",
    "    macroRecall = np.mean(recall)\n",
    "    macrof1 = np.mean(f1)\n",
    "    \n",
    "    return [[microPrec,microRecall,microf1],[macroPrec,macroRecall,macrof1]]\n",
    "\n",
    "def getConfusion(yTrue,yPred,numClass,cutoff = 0.5):\n",
    "    yPred = np.where(yPred > cutoff, 1, 0)\n",
    "    n=yTrue.shape[0]\n",
    "    assert(yTrue.shape[0]==yPred.shape[0])\n",
    "    confusion = np.zeros((numClass,numClass)).astype(int)\n",
    "    for i in range(n):\n",
    "        confusion[int(yTrue.item(i)),int(yPred.item(i))] +=1\n",
    "    return confusion\n",
    "def getAccuracy(confusion):\n",
    "    total = np.sum(np.sum(confusion))\n",
    "    tp = np.sum(confusion.diagonal())\n",
    "    return (tp/total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'19', 'Z', str(text).lower().strip())\n",
    "    text = re.sub(r'[0-9]+', ' X ', str(text).lower().strip())\n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "\n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "def processdf(df):\n",
    "    if('label' in df.columns):\n",
    "        df['label'] = np.where(df['label']==\"fake\",0,1)\n",
    "    else:\n",
    "        df['label'] = np.zeros(len(df)).astype(np.int)\n",
    "    df['tweet'] = df['tweet'].apply(utils_preprocess_text)\n",
    "    df['tweet'] = df['tweet'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textstat import flesch_reading_ease,flesch_kincaid_grade\n",
    "def getLexicon1():\n",
    "    d_emotion=pd.read_csv(\"./lexicons/5. NRC-Hashtag-Emotion-Lexicon-v0.2.txt\", sep='\\t', names=['emotion','word','score'], header=None)\n",
    "    dic_dEmo={}\n",
    "    for i in range(len(d_emotion['word'])):\n",
    "        if(type(d_emotion['word'][i]) == str):\n",
    "            turncated = d_emotion['word'][i]\n",
    "            if(turncated[0]=='#'):\n",
    "                turncated = turncated[1:]\n",
    "            dic_dEmo[turncated]=d_emotion['score'][i]\n",
    "    return dic_dEmo\n",
    "def getLexicon2():\n",
    "    d_emotion=pd.read_csv(\"./lexicons/6. NRC-10-expanded.csv\", sep='\\t')\n",
    "    dic_dEmo={}\n",
    "    for i in range(len(d_emotion['word'])):\n",
    "        if(type(d_emotion['word'][i]) == str):\n",
    "            turncated = d_emotion['word'][i]\n",
    "            if(turncated[0]=='#'):\n",
    "                turncated = turncated[1:]\n",
    "            dic_dEmo[turncated]=d_emotion['anger'][i]\n",
    "    return dic_dEmo\n",
    "def getLexicon3():\n",
    "    d_emotion=pd.read_csv(\"./lexicons/8. NRC-word-emotion-lexicon.txt\", sep='\\t', names=['word','emotion','score'], header=None)\n",
    "    dic_dEmo={}\n",
    "    for i in range(len(d_emotion['word'])):\n",
    "        if(type(d_emotion['word'][i]) == str):\n",
    "            turncated = d_emotion['word'][i]\n",
    "            if(turncated[0]=='#'):\n",
    "                turncated = turncated[1:]\n",
    "            dic_dEmo[turncated]=d_emotion['score'][i]\n",
    "    return dic_dEmo\n",
    "def vader(GivenTweets):\n",
    "    obj= SentimentIntensityAnalyzer()\n",
    "    Comp_vader=np.zeros(len(GivenTweets))\n",
    "    for i in range(len(GivenTweets)):\n",
    "        sentiment_dict = obj.polarity_scores(GivenTweets[i])\n",
    "        Comp_vader[i]=sentiment_dict['compound']\n",
    "    return(Comp_vader)\n",
    "def reading_ease(GivenTweets):\n",
    "    Reading_Ease=np.zeros(len(GivenTweets))\n",
    "    Reading_Grade=np.zeros(len(GivenTweets))\n",
    "    for i in range(len(GivenTweets)):\n",
    "        joinedString = \"\".join(GivenTweets[i])\n",
    "#         print(GivenTweets[i])\n",
    "        Reading_Ease[i]=flesch_reading_ease(joinedString)\n",
    "        Reading_Grade[i]=flesch_kincaid_grade(joinedString)\n",
    "    return(Reading_Ease,Reading_Grade)\n",
    "def slang(GivenTweets):\n",
    "    slangs= pd.read_csv(\"./lexicons/SlangSD/SlangSD.txt\", sep='\\t', names=['word','score'], header=None)\n",
    "    slangScore=np.zeros(len(GivenTweets))\n",
    "    dic_slangs={}\n",
    "    for i in (range(len(slangs['word']))):\n",
    "        dic_slangs[slangs['word'][i]]=slangs['score'][i]\n",
    "    # print(dic_slangs)\n",
    "    for i in (range(len(GivenTweets))):\n",
    "        tweet=GivenTweets[i]\n",
    "        for j in range(len(tweet)):\n",
    "            if(tweet[j] in dic_slangs):\n",
    "    # #             print(i,dic_emoji[i])\n",
    "                slangScore[i]+=dic_slangs[tweet[j]]\n",
    "    return(slangScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "def saveWord2VecModel(corpusList):\n",
    "    corpus = pd.concat([x['tweet'] for x in corpusList])\n",
    "    # WORD2VEC\n",
    "    vecmodel = gensim.models.word2vec.Word2Vec(corpus, size=VECTOR_SIZE,window=8, min_count=1, sg=1, iter=50)\n",
    "    # FASTTEXT\n",
    "    # vecmodel = FastText(corpus, size=VECTOR_SIZE, window=8, min_count=5, workers=4,sg=1)\n",
    "    open(\"word2Vec\",'wb').write(pickle.dumps(vecmodel))\n",
    "    return vecmodel\n",
    "\n",
    "VECTOR_SIZE = 200\n",
    "EXTRA_VECTORS = 3\n",
    "PADDING_SIZE = 40\n",
    "SENT_LEVEL_FEATURES = 4\n",
    "\n",
    "def getEmbeddingsFeatures(corpusList,vecmodel):\n",
    "    for i in corpusList:\n",
    "        corpus = pd.concat([x['tweet'] for x in corpusList])\n",
    "    \n",
    "    # Index words with corresponding index number and fit on corpus\n",
    "    tk=tf.keras.preprocessing.text.Tokenizer(lower=True,split=' ',oov_token=\"NaN\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    tk.fit_on_texts(corpus)\n",
    "\n",
    "    # Covert sentences to index sequences\n",
    "    for df in corpusList:\n",
    "        df['tweet_id'] = tk.texts_to_sequences(df['tweet'])\n",
    "\n",
    "    # Pad sequences\n",
    "    Xlist = [] \n",
    "    for df in corpusList:\n",
    "        Xlist.append(tf.keras.preprocessing.sequence.pad_sequences(df['tweet_id'], maxlen=PADDING_SIZE,padding=\"post\", truncating=\"post\",value=0))\n",
    "\n",
    "    ylist = []\n",
    "    for df in corpusList:\n",
    "        ylist.append(np.array(df['label']).reshape((len(df),1)))\n",
    "        \n",
    "    # EMBEDDING START\n",
    "    # ====================================================================================\n",
    "    # print(tk.word_index)\n",
    "    # LEXICON FEATURES APPEND\n",
    "    dic1 = getLexicon1()\n",
    "    dic2 = getLexicon2()\n",
    "    dic3 = getLexicon3()\n",
    "\n",
    "    embed = np.zeros((len(tk.word_index)+1, VECTOR_SIZE+EXTRA_VECTORS))\n",
    "    for word,idx in tk.word_index.items():\n",
    "        if(word in vecmodel.wv):\n",
    "    #         embed[idx] = vecmodel.wv[word]\n",
    "            lexiconVec = np.zeros(EXTRA_VECTORS)\n",
    "            if(word in dic1):\n",
    "                lexiconVec[0] = dic1[word]\n",
    "            if(word in dic2):\n",
    "                lexiconVec[1] = dic2[word]\n",
    "            if(word in dic3):\n",
    "                lexiconVec[2] = dic3[word]\n",
    "            embed[idx] = np.concatenate((vecmodel.wv[word],lexiconVec))\n",
    "    # ====================================================================================\n",
    "    # EMBEDDING END\n",
    "    \n",
    "    for i in range(len(Xlist)):\n",
    "        print(Xlist[i].shape,ylist[i].shape)\n",
    "    \n",
    "    return Xlist,ylist,embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(inputs, neurons):\n",
    "    x = keras.layers.Permute((2,1))(inputs)\n",
    "    x = keras.layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = keras.layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = keras.layers.add([inputs, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import GRU\n",
    "import keras.backend as kb\n",
    "\n",
    "def trainModelCNN(Xtrain,embed,ytrain):\n",
    "    with tf.device('/CPU:0'):\n",
    "        x_in = keras.layers.Input(shape=(PADDING_SIZE,))\n",
    "        \n",
    "        x = Embedding(input_dim=embed.shape[0],output_dim=embed.shape[1],weights=[embed],input_length=PADDING_SIZE,trainable=False)(x_in)\n",
    "        x1 = Conv1D(50,2, activation='relu',input_shape=(PADDING_SIZE,VECTOR_SIZE+EXTRA_VECTORS),padding='same') (x)\n",
    "        x1 = MaxPooling1D(pool_size=2) (x1)\n",
    "        x2 = Conv1D(50,3, activation='relu',input_shape=(PADDING_SIZE,VECTOR_SIZE+EXTRA_VECTORS),padding='same') (x)\n",
    "        x2 = MaxPooling1D(pool_size=2) (x2)\n",
    "        x3 = Conv1D(50,4, activation='relu',input_shape=(PADDING_SIZE,VECTOR_SIZE+EXTRA_VECTORS),padding='same') (x)\n",
    "        x3 = MaxPooling1D(pool_size=2) (x3)\n",
    "        x4 = Conv1D(50,5, activation='relu',input_shape=(PADDING_SIZE,VECTOR_SIZE+EXTRA_VECTORS),padding='same') (x)\n",
    "        x4 = MaxPooling1D(pool_size=2) (x4)\n",
    "#         x = Concatenate(axis=2)([x1,x2,x3,x4])\n",
    "\n",
    "        x = attention_layer(x, neurons=PADDING_SIZE)\n",
    "        x = Bidirectional(keras.layers.LSTM(units=40, dropout=0.2, return_sequences=True))(x)\n",
    "        x = Bidirectional(keras.layers.LSTM(units=40, dropout=0.2))(x)\n",
    "        \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        \n",
    "        y_out = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = keras.models.Model(x_in, y_out)\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "\n",
    "        training = model.fit(Xtrain,ytrain,batch_size=64,epochs=10, shuffle=True, verbose=1, validation_split=0.2)\n",
    "        \n",
    "        return training,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.9322429906542056\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.9526462395543176\n",
      "-----------------\n",
      "Recall: 0.9160714285714285\n",
      "-----------------\n",
      "F1 Score: 0.9340009103322713\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.9115710253998118\n",
      "-----------------\n",
      "Recall: 0.95\n",
      "-----------------\n",
      "F1 Score: 0.9303888622179549\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.932194886275113\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative                 969                  51\n",
      "Actual Positive                  94                1026\n",
      "Accuracy: 93.22429906542055\n",
      "F1 Score 0.9322429906542056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# test=pd.DataFrame(pd.read_csv(\"Constraint_English_Test - Sheet1.csv\"))\n",
    "# train=pd.DataFrame(pd.read_csv(\"Constraint_English_Train - Sheet1.csv\"))\n",
    "# val=pd.DataFrame(pd.read_csv(\"Constraint_English_Val - Sheet1.csv\"))\n",
    "\n",
    "# processdf(test)\n",
    "# processdf(train)\n",
    "# processdf(val)\n",
    "\n",
    "# vecmodel = saveWord2VecModel([train,val,test])\n",
    "\n",
    "# vecmodel = pickle.loads(open('word2Vec','rb').read())\n",
    "# Xlist,ylist,embed = getEmbeddingsFeatures([train,val,test],vecmodel)\n",
    "\n",
    "# training,model = trainModelCNN(Xlist[0],embed,ylist[0])\n",
    "\n",
    "y_pred = model.predict(Xlist[1])\n",
    "yPred = np.where(y_pred > 0.8, 1, 0)\n",
    "func_Eval(ylist[1].flatten(),yPred.flatten())\n",
    "print(\"F1 Score\",getF1(getConfusion(ylist[1],y_pred,2,cutoff=0.8))[0][2])\n",
    "# for i in range(1,100):\n",
    "#     cutoff = (1/100) * i\n",
    "#     print(\"F1 Score\",getF1(getConfusion(ylist[1],y_pred,2,cutoff=cutoff))[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(Xlist[2])\n",
    "yPred = np.where(y_pred > 0.8, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[our, daily, update, is, published, state, rep...</td>\n",
       "      <td>0</td>\n",
       "      <td>[36, 97, 73, 10, 170, 23, 49, 2, 72, 21, 2, 72...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[alfalfa, is, the, only, cure, for, covidz]</td>\n",
       "      <td>0</td>\n",
       "      <td>[21455, 10, 3, 99, 158, 12, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[president, trump, asked, what, he, would, do,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[139, 93, 765, 213, 108, 230, 114, 66, 108, 61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[state, reported, X, death, we, are, still, se...</td>\n",
       "      <td>0</td>\n",
       "      <td>[23, 49, 2, 27, 25, 14, 191, 1109, 6, 2204, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[this, is, the, sixth, time, a, global, health...</td>\n",
       "      <td>0</td>\n",
       "      <td>[29, 10, 3, 3169, 87, 6, 416, 57, 467, 22, 38,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>2136</td>\n",
       "      <td>[coronavirusupdates, statewise, detail, of, to...</td>\n",
       "      <td>0</td>\n",
       "      <td>[128, 1225, 161, 4, 40, 45, 8, 11, 545, 2, 215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>2137</td>\n",
       "      <td>[tonight, X, midnight, onwards, disaster, mana...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2010, 2, 4094, 24623, 1248, 623, 506, 22, 38,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>2138</td>\n",
       "      <td>[X, new, case, of, covidznigeria, plateau, X, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 18, 11, 4, 258, 295, 2, 361, 2, 224, 2, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>2139</td>\n",
       "      <td>[rt, cdcemergency, dyk, cdcgovs, onestop, shop...</td>\n",
       "      <td>0</td>\n",
       "      <td>[82, 1442, 1405, 24625, 6137, 1244, 12, 8, 983...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>2140</td>\n",
       "      <td>[more, than, half, of, pregnant, woman, recent...</td>\n",
       "      <td>0</td>\n",
       "      <td>[30, 50, 733, 4, 1409, 287, 797, 1814, 5, 6, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2140 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  label  \\\n",
       "0        1  [our, daily, update, is, published, state, rep...      0   \n",
       "1        2        [alfalfa, is, the, only, cure, for, covidz]      0   \n",
       "2        3  [president, trump, asked, what, he, would, do,...      0   \n",
       "3        4  [state, reported, X, death, we, are, still, se...      0   \n",
       "4        5  [this, is, the, sixth, time, a, global, health...      0   \n",
       "...    ...                                                ...    ...   \n",
       "2135  2136  [coronavirusupdates, statewise, detail, of, to...      0   \n",
       "2136  2137  [tonight, X, midnight, onwards, disaster, mana...      0   \n",
       "2137  2138  [X, new, case, of, covidznigeria, plateau, X, ...      0   \n",
       "2138  2139  [rt, cdcemergency, dyk, cdcgovs, onestop, shop...      0   \n",
       "2139  2140  [more, than, half, of, pregnant, woman, recent...      0   \n",
       "\n",
       "                                               tweet_id  \n",
       "0     [36, 97, 73, 10, 170, 23, 49, 2, 72, 21, 2, 72...  \n",
       "1                        [21455, 10, 3, 99, 158, 12, 8]  \n",
       "2     [139, 93, 765, 213, 108, 230, 114, 66, 108, 61...  \n",
       "3     [23, 49, 2, 27, 25, 14, 191, 1109, 6, 2204, 20...  \n",
       "4     [29, 10, 3, 3169, 87, 6, 416, 57, 467, 22, 38,...  \n",
       "...                                                 ...  \n",
       "2135  [128, 1225, 161, 4, 40, 45, 8, 11, 545, 2, 215...  \n",
       "2136  [2010, 2, 4094, 24623, 1248, 623, 506, 22, 38,...  \n",
       "2137  [2, 18, 11, 4, 258, 295, 2, 361, 2, 224, 2, 14...  \n",
       "2138  [82, 1442, 1405, 24625, 6137, 1244, 12, 8, 983...  \n",
       "2139  [30, 50, 733, 4, 1409, 287, 797, 1814, 5, 6, 2...  \n",
       "\n",
       "[2140 rows x 4 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers.txt\", \"a\")\n",
    "f.write(\"id,label\\n\")\n",
    "c=1\n",
    "for i in range(len(yPred.flatten())):\n",
    "    if(yPred[i]==0):\n",
    "        f.write(str(i+1)+\",\"+\"fake\")\n",
    "    else:\n",
    "        f.write(str(i+1)+\",\"+\"real\")\n",
    "    if(i!=len(yPred.flatten())-1):\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_Eval(Y_Actual,Y_pred):\n",
    "\n",
    "    TrueNegative=0\n",
    "    TruePositive=0\n",
    "    FalsePositive=0\n",
    "    FalseNegative=0\n",
    "    for i in range(len(Y_pred)):\n",
    "        #if the actual class is negative\n",
    "        if(Y_Actual[i]==0):\n",
    "            \n",
    "            #if the predicited class is negative\n",
    "            if(Y_pred[i]==0):\n",
    "                TrueNegative+=1\n",
    "                \n",
    "            #if the predicted class is positive\n",
    "            else:\n",
    "                FalsePositive+=1\n",
    "                \n",
    "        #if the actual class is positive\n",
    "        else:\n",
    "            #if the predicited class is positive\n",
    "            if(Y_pred[i]==1):\n",
    "                TruePositive+=1\n",
    "                \n",
    "            #if the predicited class is negative\n",
    "            else:\n",
    "                FalseNegative+=1\n",
    "\n",
    "    Confusion_Matrix=[[TrueNegative,FalsePositive],[FalseNegative,TruePositive]]\n",
    "    \n",
    "    Confusion_Matrix=pd.DataFrame(Confusion_Matrix,columns=['Predicted Negative','Predicted Positive'])\n",
    "    \n",
    "    Confusion_Matrix.rename(index={0: \"Actual Negative\", 1: \"Actual Positive\"},inplace=True)\n",
    "    \n",
    "    MyPrecision=TruePositive/(TruePositive+FalsePositive)\n",
    "\n",
    "    MyRecall=TruePositive/(TruePositive+FalseNegative)\n",
    "    \n",
    "    MyAccuracy=(TruePositive+TrueNegative)/(TruePositive+TrueNegative+FalseNegative+FalsePositive)\n",
    "    \n",
    "    MyF1score= 2*(MyPrecision*MyRecall)/(MyPrecision+MyRecall)\n",
    "    \n",
    "    MyPrecisionZero=TrueNegative/(TrueNegative+FalseNegative)\n",
    "\n",
    "    MyRecallZero=TrueNegative/(TrueNegative+FalsePositive)\n",
    "    \n",
    "    \n",
    "    MyF1scoreZero= 2*(MyPrecisionZero*MyRecallZero)/(MyPrecisionZero+MyRecallZero)\n",
    "\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "    print(\"Accuracy:\",MyAccuracy)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Positive CLass, Label:4\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecision)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecall)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1score)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Negative CLass, Label:0\")\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecisionZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecallZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1scoreZero)\n",
    "\n",
    "    print()\n",
    "    print(\"--------\")\n",
    "    print(\"Macro Average F1\",(MyF1score+MyF1scoreZero)/2)\n",
    "    print(\"-----------------\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(Confusion_Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                      | 0/10700 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\confusement\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████| 10700/10700 [00:20<00:00, 528.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>decoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the cdc currently reports X deaths in general ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 1996, 26629, 2747, 4311, 1060, 6677, 199...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] the cdc currently reports x deaths in ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>states reported X deaths a small rise from las...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 2163, 2988, 1060, 6677, 1037, 2235, 4125...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] states reported x deaths a small rise fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>politically correct woman almost uses pandemic...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[101, 10317, 6149, 2450, 2471, 3594, 6090, 320...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] politically correct woman almost uses pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>indiafightscorona we have X covid testing labo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 2634, 20450, 9363, 26788, 2057, 2031, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] indiafightscorona we have x covid testin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>populous states can generate large case counts...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 20151, 2163, 2064, 9699, 2312, 2553, 929...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] populous states can generate large case ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>2136</td>\n",
       "      <td>donald trump wrongly claimed that new zealand ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[101, 6221, 8398, 29116, 3555, 2008, 2047, 341...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] donald trump wrongly claimed that new ze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>2137</td>\n",
       "      <td>current understanding is covidz spreads mostly...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 2783, 4824, 2003, 2522, 17258, 2480, 208...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] current understanding is covidz spreads ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>2138</td>\n",
       "      <td>nothing screams i am sat around doing fuck all...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[101, 2498, 11652, 1045, 2572, 2938, 2105, 272...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] nothing screams i am sat around doing fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>2139</td>\n",
       "      <td>birx says covidz outbreak not under control be...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[101, 12170, 2099, 2595, 2758, 2522, 17258, 24...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] birx says covidz outbreak not under cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>2140</td>\n",
       "      <td>another X new coronavirus cases have been conf...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 2178, 1060, 2047, 21887, 23350, 3572, 20...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] another x new coronavirus cases have bee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10700 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  label  \\\n",
       "0        1  the cdc currently reports X deaths in general ...    1.0   \n",
       "1        2  states reported X deaths a small rise from las...    1.0   \n",
       "2        3  politically correct woman almost uses pandemic...    0.0   \n",
       "3        4  indiafightscorona we have X covid testing labo...    1.0   \n",
       "4        5  populous states can generate large case counts...    1.0   \n",
       "...    ...                                                ...    ...   \n",
       "2135  2136  donald trump wrongly claimed that new zealand ...    0.0   \n",
       "2136  2137  current understanding is covidz spreads mostly...    1.0   \n",
       "2137  2138  nothing screams i am sat around doing fuck all...    0.0   \n",
       "2138  2139  birx says covidz outbreak not under control be...    0.0   \n",
       "2139  2140  another X new coronavirus cases have been conf...    1.0   \n",
       "\n",
       "                                              input_ids  \\\n",
       "0     [101, 1996, 26629, 2747, 4311, 1060, 6677, 199...   \n",
       "1     [101, 2163, 2988, 1060, 6677, 1037, 2235, 4125...   \n",
       "2     [101, 10317, 6149, 2450, 2471, 3594, 6090, 320...   \n",
       "3     [101, 2634, 20450, 9363, 26788, 2057, 2031, 10...   \n",
       "4     [101, 20151, 2163, 2064, 9699, 2312, 2553, 929...   \n",
       "...                                                 ...   \n",
       "2135  [101, 6221, 8398, 29116, 3555, 2008, 2047, 341...   \n",
       "2136  [101, 2783, 4824, 2003, 2522, 17258, 2480, 208...   \n",
       "2137  [101, 2498, 11652, 1045, 2572, 2938, 2105, 272...   \n",
       "2138  [101, 12170, 2099, 2595, 2758, 2522, 17258, 24...   \n",
       "2139  [101, 2178, 1060, 2047, 21887, 23350, 3572, 20...   \n",
       "\n",
       "                                            segment_ids  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2135  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2136  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2137  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2138  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2139  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         attention_mask  \\\n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "2135  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2136  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2137  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2138  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2139  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                decoded  \n",
       "0     [CLS] the cdc currently reports x deaths in ge...  \n",
       "1     [CLS] states reported x deaths a small rise fr...  \n",
       "2     [CLS] politically correct woman almost uses pa...  \n",
       "3     [CLS] indiafightscorona we have x covid testin...  \n",
       "4     [CLS] populous states can generate large case ...  \n",
       "...                                                 ...  \n",
       "2135  [CLS] donald trump wrongly claimed that new ze...  \n",
       "2136  [CLS] current understanding is covidz spreads ...  \n",
       "2137  [CLS] nothing screams i am sat around doing fu...  \n",
       "2138  [CLS] birx says covidz outbreak not under cont...  \n",
       "2139  [CLS] another x new coronavirus cases have bee...  \n",
       "\n",
       "[10700 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PADDING_SIZE = 30\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "corpus = pd.concat([train,test,val])\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "segment_ids = []\n",
    "decoded=[]\n",
    "for i in tqdm(range(len(corpus))):\n",
    "    bert_inp= bert_tokenizer.encode_plus(corpus.iloc[i]['tweet'],add_special_tokens=True,max_length =PADDING_SIZE,pad_to_max_length = True,return_attention_mask=True)\n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "    decoded.append(bert_tokenizer.decode(bert_inp['input_ids']))\n",
    "    segment_ids.append([0]*PADDING_SIZE)\n",
    "corpus['input_ids'] = input_ids\n",
    "corpus['segment_ids'] = segment_ids\n",
    "corpus['attention_mask'] = attention_masks\n",
    "corpus['decoded'] = decoded\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 30) (8560, 30) (8560, 30)\n",
      "(6420, 30) (8560, 30) (8560, 30)\n",
      "(6420, 30) (8560, 30) (8560, 30)\n",
      "(6420,) (2140,)\n"
     ]
    }
   ],
   "source": [
    "train1 = corpus.iloc[:6420]\n",
    "test1 = corpus.iloc[:8560]\n",
    "val1 = corpus.iloc[8560:]\n",
    "\n",
    "Xtrain_id = np.vstack(train1['input_ids'])\n",
    "Xtrain_att = np.vstack(train1['attention_mask'])\n",
    "Xtrain_seg = np.vstack(train1['segment_ids'])\n",
    "\n",
    "Xtest_id = np.vstack(test1['input_ids'])\n",
    "Xtest_att = np.vstack(test1['attention_mask'])\n",
    "Xtest_seg = np.vstack(test1['segment_ids'])\n",
    "\n",
    "Xval_id = np.vstack(test1['input_ids'])\n",
    "Xval_att = np.vstack(test1['attention_mask'])\n",
    "Xval_seg = np.vstack(test1['segment_ids'])\n",
    "\n",
    "trainComb = (Xtrain_id,Xtrain_att,Xtrain_seg)\n",
    "valComb = (Xval_id,Xval_att,Xval_seg)\n",
    "\n",
    "ytrain = np.array(train1['label'])\n",
    "yval = np.array(val1['label'])\n",
    "\n",
    "print(Xtrain_id.shape,Xtest_id.shape,Xval_id.shape)\n",
    "print(Xtrain_att.shape,Xtest_att.shape,Xval_att.shape)\n",
    "print(Xtrain_seg.shape,Xtest_seg.shape,Xval_seg.shape)\n",
    "\n",
    "print(ytrain.shape,yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "\n",
    "#MODEL 2\n",
    "input_word_ids = tf.keras.Input(shape=(PADDING_SIZE,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = tf.keras.Input(shape=(PADDING_SIZE,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.keras.Input(shape=(PADDING_SIZE,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "clf_output = sequence_output[:, 0, :]\n",
    "net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
    "net = tf.keras.layers.Dropout(0.2)(net)\n",
    "# net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "# net = tf.keras.layers.Dropout(0.2)(net)\n",
    "out = tf.keras.layers.Dense(1, activation='softmax')(net)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#FIT\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "train_history = model.fit(trainComb,ytrain, validation_split=0.2,epochs=1,callbacks=[checkpoint, earlystopping],batch_size=256,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.7423 - binary_accuracy: 0.4961\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.7423 - binary_accuracy: 0.4961\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 7.7423 - binary_accuracy: 0.4961\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 7.7423 - binary_accuracy: 0.4961\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.7423 - binary_accuracy: 0.4961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2f0010d7220>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(10, input_shape=(30,)))\n",
    "model.add(keras.layers.Dense(1, input_shape=(30,)))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "model.fit(x=Xtrain_id,y=ytrain,shuffle=True,epochs=5,batch_size=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
