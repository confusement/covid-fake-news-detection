{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "#Word2Vec\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "#Navneet\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_Eval(Y_Actual,Y_pred):\n",
    "\n",
    "    TrueNegative=0\n",
    "    TruePositive=0\n",
    "    FalsePositive=0\n",
    "    FalseNegative=0\n",
    "    for i in range(len(Y_pred)):\n",
    "        #if the actual class is negative\n",
    "        if(Y_Actual[i]==0):\n",
    "            \n",
    "            #if the predicited class is negative\n",
    "            if(Y_pred[i]==0):\n",
    "                TrueNegative+=1\n",
    "                \n",
    "            #if the predicted class is positive\n",
    "            else:\n",
    "                FalsePositive+=1\n",
    "                \n",
    "        #if the actual class is positive\n",
    "        else:\n",
    "            #if the predicited class is positive\n",
    "            if(Y_pred[i]==1):\n",
    "                TruePositive+=1\n",
    "                \n",
    "            #if the predicited class is negative\n",
    "            else:\n",
    "                FalseNegative+=1\n",
    "\n",
    "    Confusion_Matrix=[[TrueNegative,FalsePositive],[FalseNegative,TruePositive]]\n",
    "    \n",
    "    Confusion_Matrix=pd.DataFrame(Confusion_Matrix,columns=['Predicted Negative','Predicted Positive'])\n",
    "    \n",
    "    Confusion_Matrix.rename(index={0: \"Actual Negative\", 1: \"Actual Positive\"},inplace=True)\n",
    "    \n",
    "    MyPrecision=TruePositive/(TruePositive+FalsePositive)\n",
    "\n",
    "    MyRecall=TruePositive/(TruePositive+FalseNegative)\n",
    "    \n",
    "    MyAccuracy=(TruePositive+TrueNegative)/(TruePositive+TrueNegative+FalseNegative+FalsePositive)\n",
    "    \n",
    "    MyF1score= 2*(MyPrecision*MyRecall)/(MyPrecision+MyRecall)\n",
    "    \n",
    "    MyPrecisionZero=TrueNegative/(TrueNegative+FalseNegative)\n",
    "\n",
    "    MyRecallZero=TrueNegative/(TrueNegative+FalsePositive)\n",
    "    \n",
    "    \n",
    "    MyF1scoreZero= 2*(MyPrecisionZero*MyRecallZero)/(MyPrecisionZero+MyRecallZero)\n",
    "\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "    print(\"Accuracy:\",MyAccuracy)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Positive CLass, Label:4\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecision)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecall)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1score)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Negative CLass, Label:0\")\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecisionZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecallZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1scoreZero)\n",
    "\n",
    "    print()\n",
    "    print(\"--------\")\n",
    "    print(\"Macro Average F1\",(MyF1score+MyF1scoreZero)/2)\n",
    "    print(\"-----------------\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(Confusion_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getF1(confusion):\n",
    "    print(\"Accuracy:\", getAccuracy(confusion))\n",
    "    numClass = confusion.shape[0]\n",
    "    total = np.sum(np.sum(confusion))\n",
    "    \n",
    "    # get True positive from diagonals\n",
    "    tp = confusion.diagonal().reshape((numClass,1)).copy()\n",
    "    \n",
    "    # make diagonal zeros\n",
    "    confusionCopy = confusion.copy()\n",
    "    np.fill_diagonal(confusionCopy,0)\n",
    "    fn = np.sum(confusionCopy,axis=1).reshape(((numClass,1))).copy()\n",
    "    fp = np.sum(confusionCopy,axis=0).reshape(((numClass,1))).copy()\n",
    "    if(np.sum(tp+fp)==0 or np.isfinite(tp+fp).all()==False):\n",
    "        print(tp,fp)\n",
    "        prec = tp\n",
    "    else:\n",
    "        prec = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    f1 = (2*prec*recall) / (prec+recall)\n",
    "    \n",
    "    microPrec = np.sum(tp) / (np.sum(tp)+np.sum(fp))\n",
    "    microRecall = np.sum(tp) / (np.sum(tp)+np.sum(fn))\n",
    "    microf1 = (2*microPrec*microRecall) / (microPrec+microRecall)\n",
    "    \n",
    "    macroPrec = np.mean(prec)\n",
    "    macroRecall = np.mean(recall)\n",
    "    macrof1 = np.mean(f1)\n",
    "    \n",
    "    return [[microPrec,microRecall,microf1],[macroPrec,macroRecall,macrof1]]\n",
    "\n",
    "def getConfusion(yTrue,yPred,numClass,cutoff = 0.5):\n",
    "    yPred = np.where(yPred > cutoff, 1, 0)\n",
    "    n=yTrue.shape[0]\n",
    "    assert(yTrue.shape[0]==yPred.shape[0])\n",
    "    confusion = np.zeros((numClass,numClass)).astype(int)\n",
    "    for i in range(n):\n",
    "        confusion[int(yTrue.item(i)),int(yPred.item(i))] +=1\n",
    "    return confusion\n",
    "def getAccuracy(confusion):\n",
    "    total = np.sum(np.sum(confusion))\n",
    "    tp = np.sum(confusion.diagonal())\n",
    "    return (tp/total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'19', 'Z', str(text).lower().strip())\n",
    "    text = re.sub(r'[0-9]+k*', ' X ', str(text).lower().strip())\n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "\n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "def processdf(df):\n",
    "    if('label' in df.columns):\n",
    "        df['label'] = np.where(df['label']==\"fake\",0,1)\n",
    "    else:\n",
    "        df['label'] = np.zeros(len(df)).astype(np.int)\n",
    "    df['tweet'] = df['tweet'].apply(utils_preprocess_text)\n",
    "    df['tweet'] = df['tweet'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textstat import flesch_reading_ease,flesch_kincaid_grade\n",
    "def getLexicon1():\n",
    "    d_emotion=pd.read_csv(\"./lexicons/5. NRC-Hashtag-Emotion-Lexicon-v0.2.txt\", sep='\\t', names=['emotion','word','score'], header=None)\n",
    "    dic_dEmo={}\n",
    "    for i in range(len(d_emotion['word'])):\n",
    "        if(type(d_emotion['word'][i]) == str):\n",
    "            turncated = d_emotion['word'][i]\n",
    "            if(turncated[0]=='#'):\n",
    "                turncated = turncated[1:]\n",
    "            dic_dEmo[turncated]=d_emotion['score'][i]\n",
    "    return dic_dEmo\n",
    "def getLexicon2():\n",
    "    d_emotion=pd.read_csv(\"./lexicons/6. NRC-10-expanded.csv\", sep='\\t')\n",
    "    dic_dEmo={}\n",
    "    for i in range(len(d_emotion['word'])):\n",
    "        if(type(d_emotion['word'][i]) == str):\n",
    "            turncated = d_emotion['word'][i]\n",
    "            if(turncated[0]=='#'):\n",
    "                turncated = turncated[1:]\n",
    "            dic_dEmo[turncated]=d_emotion['anger'][i]\n",
    "    return dic_dEmo\n",
    "def getLexicon3():\n",
    "    d_emotion=pd.read_csv(\"./lexicons/8. NRC-word-emotion-lexicon.txt\", sep='\\t', names=['word','emotion','score'], header=None)\n",
    "    dic_dEmo={}\n",
    "    for i in range(len(d_emotion['word'])):\n",
    "        if(type(d_emotion['word'][i]) == str):\n",
    "            turncated = d_emotion['word'][i]\n",
    "            if(turncated[0]=='#'):\n",
    "                turncated = turncated[1:]\n",
    "            dic_dEmo[turncated]=d_emotion['score'][i]\n",
    "    return dic_dEmo\n",
    "def vader(GivenTweets):\n",
    "    obj= SentimentIntensityAnalyzer()\n",
    "    Comp_vader=np.zeros(len(GivenTweets))\n",
    "    for i in range(len(GivenTweets)):\n",
    "        sentiment_dict = obj.polarity_scores(GivenTweets[i])\n",
    "        Comp_vader[i]=sentiment_dict['compound']\n",
    "    return(Comp_vader)\n",
    "def reading_ease(GivenTweets):\n",
    "    Reading_Ease=np.zeros(len(GivenTweets))\n",
    "    Reading_Grade=np.zeros(len(GivenTweets))\n",
    "    for i in range(len(GivenTweets)):\n",
    "        joinedString = \"\".join(GivenTweets[i])\n",
    "#         print(GivenTweets[i])\n",
    "        Reading_Ease[i]=flesch_reading_ease(joinedString)\n",
    "        Reading_Grade[i]=flesch_kincaid_grade(joinedString)\n",
    "    return(Reading_Ease,Reading_Grade)\n",
    "def slang(GivenTweets):\n",
    "    slangs= pd.read_csv(\"./lexicons/SlangSD/SlangSD.txt\", sep='\\t', names=['word','score'], header=None)\n",
    "    slangScore=np.zeros(len(GivenTweets))\n",
    "    dic_slangs={}\n",
    "    for i in (range(len(slangs['word']))):\n",
    "        dic_slangs[slangs['word'][i]]=slangs['score'][i]\n",
    "    # print(dic_slangs)\n",
    "    for i in (range(len(GivenTweets))):\n",
    "        tweet=GivenTweets[i]\n",
    "        for j in range(len(tweet)):\n",
    "            if(tweet[j] in dic_slangs):\n",
    "    # #             print(i,dic_emoji[i])\n",
    "                slangScore[i]+=dic_slangs[tweet[j]]\n",
    "    return(slangScore)\n",
    "def getLength(GivenTweets):\n",
    "    ret=np.zeros(len(GivenTweets))\n",
    "    for i in (range(len(GivenTweets))):\n",
    "        tweet=GivenTweets[i]\n",
    "        ret[i] = len(tweet)\n",
    "    return ret\n",
    "def getAvgWord(GivenTweets):\n",
    "    ret=np.zeros(len(GivenTweets))\n",
    "    for i in (range(len(GivenTweets))):\n",
    "        tweet=GivenTweets[i]\n",
    "        cnt = 0\n",
    "        add = 0\n",
    "        for j in range(len(tweet)):\n",
    "            cnt+=1\n",
    "            add+= len(tweet[j])\n",
    "        ret[i] = add/cnt\n",
    "    return ret\n",
    "def getSpecial(GivenTweets):\n",
    "    ret=np.zeros(len(GivenTweets))\n",
    "    for i in (range(len(GivenTweets))):\n",
    "        tweet=GivenTweets[i]\n",
    "        cnt = 0\n",
    "        for j in range(len(tweet)):\n",
    "            if(tweet[j]=='X'):\n",
    "                cnt+=1\n",
    "        ret[i] = cnt\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def helpergun(x):\n",
    "    return \" \".join(x)\n",
    "def saveWord2VecModel(corpusList):\n",
    "    corpus = pd.concat([x['tweet'] for x in corpusList])\n",
    "    # WORD2VEC\n",
    "    vecmodel = gensim.models.word2vec.Word2Vec(corpus, size=VECTOR_SIZE,window=8, min_count=1, sg=1, iter=50)\n",
    "    # FASTTEXT\n",
    "    # vecmodel = FastText(corpus, size=VECTOR_SIZE, window=8, min_count=5, workers=4,sg=1)\n",
    "    open(\"word2Vec\",'wb').write(pickle.dumps(vecmodel))\n",
    "    return vecmodel\n",
    "\n",
    "VECTOR_SIZE = 200\n",
    "EXTRA_VECTORS = 3\n",
    "PADDING_SIZE = 40\n",
    "SENT_LEVEL_FEATURES = 5\n",
    "UNIGRAMS_CNT = 4070\n",
    "\n",
    "def getEmbeddingsFeatures(corpusList,vecmodel):\n",
    "    corpus = pd.concat([x['tweet'] for x in corpusList])\n",
    "    \n",
    "    corpusJoined = corpus.apply(helpergun)\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1),min_df=5)\n",
    "    unigrams =vectorizer.fit_transform(corpusJoined)\n",
    "    \n",
    "    X_unig = []\n",
    "    for df in corpusList:\n",
    "        X_unig.append(vectorizer.transform(df['tweet'].apply(helpergun)))\n",
    "    \n",
    "    # Index words with corresponding index number and fit on corpus\n",
    "    tk=tf.keras.preprocessing.text.Tokenizer(lower=True,split=' ',oov_token=\"NaN\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    tk.fit_on_texts(corpus)\n",
    "\n",
    "    # Covert sentences to index sequences\n",
    "    for df in corpusList:\n",
    "        df['tweet_id'] = tk.texts_to_sequences(df['tweet'])\n",
    "\n",
    "    # Pad sequences\n",
    "    Xlist = [] \n",
    "    for df in corpusList:\n",
    "        Xlist.append(tf.keras.preprocessing.sequence.pad_sequences(df['tweet_id'], maxlen=PADDING_SIZE,padding=\"post\", truncating=\"post\",value=0))\n",
    "\n",
    "    ylist = []\n",
    "    for df in corpusList:\n",
    "        ylist.append(np.array(df['label']).reshape((len(df),1)))\n",
    "        \n",
    "    # EMBEDDING START\n",
    "    # ====================================================================================\n",
    "    # print(tk.word_index)\n",
    "    # LEXICON FEATURES APPEND\n",
    "    dic1 = getLexicon1()\n",
    "    dic2 = getLexicon2()\n",
    "    dic3 = getLexicon3()\n",
    "\n",
    "    embed = np.zeros((len(tk.word_index)+1, VECTOR_SIZE+EXTRA_VECTORS))\n",
    "    for word,idx in tk.word_index.items():\n",
    "        if(word in vecmodel.wv):\n",
    "    #         embed[idx] = vecmodel.wv[word]\n",
    "            lexiconVec = np.zeros(EXTRA_VECTORS)\n",
    "            if(word in dic1):\n",
    "                lexiconVec[0] = dic1[word]\n",
    "            if(word in dic2):\n",
    "                lexiconVec[1] = dic2[word]\n",
    "            if(word in dic3):\n",
    "                lexiconVec[2] = dic3[word]\n",
    "            embed[idx] = np.concatenate((vecmodel.wv[word],lexiconVec))\n",
    "    # ====================================================================================\n",
    "    # EMBEDDING END\n",
    "    \n",
    "    # SENT LEVEL FEATURES START\n",
    "    XFlist = []\n",
    "    gh = 0\n",
    "    for df in corpusList:\n",
    "        XF = np.zeros((len(df),SENT_LEVEL_FEATURES+UNIGRAMS_CNT))\n",
    "#         XF[:,0] = vader(df['tweet'])\n",
    "        re,rg = reading_ease(df['tweet'])\n",
    "        XF[:,0] = re\n",
    "        XF[:,1] = rg\n",
    "#         XF[:,3] = slang(df['tweet'])\n",
    "        XF[:,2] = getLength(df['tweet'])\n",
    "        XF[:,3] = getAvgWord(df['tweet'])\n",
    "        XF[:,4] = getSpecial(df['tweet'])\n",
    "        XF[:,5:] = X_unig[gh].toarray()\n",
    "        XFlist.append(XF)\n",
    "        gh+=1\n",
    "    \n",
    "    for i in range(len(Xlist)):\n",
    "        print(Xlist[i].shape,ylist[i].shape)\n",
    "    \n",
    "    return Xlist,ylist,XFlist,embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"train_feature\",\"rb\")\n",
    "xt=pickle.load(file)\n",
    "file.close()\n",
    "file=open(\"train_label\",\"rb\")\n",
    "yt=pickle.load(file)\n",
    "file.close()\n",
    "file=open(\"valid_label\",\"rb\")\n",
    "yv=pickle.load(file)\n",
    "file.close()\n",
    "file=open(\"valid_feature\",\"rb\")\n",
    "xv=pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file=open(\"test_feature\",\"rb\")\n",
    "testX=pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6420,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt = np.where(yt==\"real\",1,0)\n",
    "yv = np.where(yv==\"real\",1,0)\n",
    "yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = xt.tocsr()\n",
    "xv = xv.tocsr()\n",
    "testX = testX.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4544735 ],\n",
       "       [0.5367213 ],\n",
       "       [0.4613737 ],\n",
       "       ...,\n",
       "       [0.52380615],\n",
       "       [0.4835064 ],\n",
       "       [0.6042369 ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 28601)             0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 32)                915264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 915,733\n",
      "Trainable params: 915,669\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "6420/6420 [==============================] - 4s 675us/step - loss: 0.2645 - accuracy: 0.9048\n",
      "Epoch 2/10\n",
      "6420/6420 [==============================] - 4s 613us/step - loss: 0.0276 - accuracy: 0.9939\n",
      "Epoch 3/10\n",
      "6420/6420 [==============================] - 4s 615us/step - loss: 0.0045 - accuracy: 0.9997\n",
      "Epoch 4/10\n",
      "6420/6420 [==============================] - 4s 612us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6420/6420 [==============================] - 4s 644us/step - loss: 9.8962e-04 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6420/6420 [==============================] - 4s 648us/step - loss: 6.2368e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6420/6420 [==============================] - 4s 659us/step - loss: 4.4153e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6420/6420 [==============================] - 4s 643us/step - loss: 3.2818e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6420/6420 [==============================] - 4s 654us/step - loss: 2.5186e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6420/6420 [==============================] - 4s 642us/step - loss: 1.9709e-04 - accuracy: 1.0000\n",
      "MACRO F1 :  0.9731182795698925\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    x_in = keras.layers.Input(shape=(xt.shape[1],))\n",
    "    x = Dense(32, activation='relu')(x_in)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(10, activation='relu')(x)     \n",
    "    y_out = Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.models.Model(x_in,y_out)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    training = model.fit(xt,yt,batch_size=64,epochs=10, shuffle=\"batch\", verbose=1, validation_split=0)\n",
    "    y_pred = model.predict(xv)\n",
    "    y_cut = np.where(y_pred>0.5,1,0)\n",
    "    macrof1 = f1_score(yv, y_cut, zero_division=1)\n",
    "    print(\"MACRO F1 : \",macrof1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.3913 - accuracy: 0.8830 - val_loss: 0.1217 - val_accuracy: 0.9766\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0483 - accuracy: 0.9910 - val_loss: 0.0643 - val_accuracy: 0.9766\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0094 - accuracy: 0.9994 - val_loss: 0.0567 - val_accuracy: 0.9778\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0034 - accuracy: 0.9999 - val_loss: 0.0527 - val_accuracy: 0.9825\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9813\n",
      "Epoch 6/10\n",
      " - 8s - loss: 9.6766e-04 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 0.9825\n",
      "Epoch 7/10\n",
      " - 8s - loss: 6.1483e-04 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 0.9825\n",
      "Epoch 8/10\n",
      " - 8s - loss: 4.1654e-04 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 0.9825\n",
      "Epoch 9/10\n",
      " - 8s - loss: 2.9615e-04 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 0.9848\n",
      "Epoch 10/10\n",
      " - 8s - loss: 2.1833e-04 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9848\n",
      "MACRO F1 :  0.9854748603351955\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4234 - accuracy: 0.9060 - val_loss: 0.1346 - val_accuracy: 0.9661\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0577 - accuracy: 0.9899 - val_loss: 0.0688 - val_accuracy: 0.9766\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0120 - accuracy: 0.9990 - val_loss: 0.0623 - val_accuracy: 0.9743\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0042 - accuracy: 0.9997 - val_loss: 0.0631 - val_accuracy: 0.9731\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 0.9731\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9743\n",
      "Epoch 7/10\n",
      " - 8s - loss: 7.7864e-04 - accuracy: 1.0000 - val_loss: 0.0644 - val_accuracy: 0.9743\n",
      "Epoch 8/10\n",
      " - 8s - loss: 5.4295e-04 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 0.9731\n",
      "Epoch 9/10\n",
      " - 8s - loss: 3.9710e-04 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9743\n",
      "Epoch 10/10\n",
      " - 8s - loss: 3.0130e-04 - accuracy: 1.0000 - val_loss: 0.0673 - val_accuracy: 0.9731\n",
      "MACRO F1 :  0.9740112994350283\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.3959 - accuracy: 0.9097 - val_loss: 0.1491 - val_accuracy: 0.9614\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0541 - accuracy: 0.9903 - val_loss: 0.0882 - val_accuracy: 0.9673\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0109 - accuracy: 0.9991 - val_loss: 0.0847 - val_accuracy: 0.9673\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0035 - accuracy: 0.9999 - val_loss: 0.0800 - val_accuracy: 0.9731\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9743\n",
      "Epoch 6/10\n",
      " - 8s - loss: 8.4193e-04 - accuracy: 1.0000 - val_loss: 0.0821 - val_accuracy: 0.9743\n",
      "Epoch 7/10\n",
      " - 8s - loss: 5.3427e-04 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9743\n",
      "Epoch 8/10\n",
      " - 8s - loss: 3.6579e-04 - accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 0.9731\n",
      "Epoch 9/10\n",
      " - 8s - loss: 2.6443e-04 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 0.9731\n",
      "Epoch 10/10\n",
      " - 8s - loss: 1.9860e-04 - accuracy: 1.0000 - val_loss: 0.0868 - val_accuracy: 0.9731\n",
      "MACRO F1 :  0.9746974697469747\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4141 - accuracy: 0.8651 - val_loss: 0.1597 - val_accuracy: 0.9544\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0549 - accuracy: 0.9916 - val_loss: 0.0846 - val_accuracy: 0.9638\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0110 - accuracy: 0.9991 - val_loss: 0.0787 - val_accuracy: 0.9708\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0042 - accuracy: 0.9999 - val_loss: 0.0772 - val_accuracy: 0.9673\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.0784 - val_accuracy: 0.9685\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0793 - val_accuracy: 0.9696\n",
      "Epoch 7/10\n",
      " - 8s - loss: 7.7361e-04 - accuracy: 1.0000 - val_loss: 0.0800 - val_accuracy: 0.9696\n",
      "Epoch 8/10\n",
      " - 8s - loss: 5.2318e-04 - accuracy: 1.0000 - val_loss: 0.0816 - val_accuracy: 0.9696\n",
      "Epoch 9/10\n",
      " - 8s - loss: 3.6944e-04 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9708\n",
      "Epoch 10/10\n",
      " - 8s - loss: 2.7211e-04 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 0.9708\n",
      "MACRO F1 :  0.9733759318423855\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4098 - accuracy: 0.8627 - val_loss: 0.1373 - val_accuracy: 0.9685\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0531 - accuracy: 0.9896 - val_loss: 0.0813 - val_accuracy: 0.9743\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0100 - accuracy: 0.9995 - val_loss: 0.0759 - val_accuracy: 0.9755\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0758 - val_accuracy: 0.9766\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0774 - val_accuracy: 0.9766\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0793 - val_accuracy: 0.9766\n",
      "Epoch 7/10\n",
      " - 8s - loss: 7.8158e-04 - accuracy: 1.0000 - val_loss: 0.0809 - val_accuracy: 0.9766\n",
      "Epoch 8/10\n",
      " - 8s - loss: 5.5424e-04 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 0.9778\n",
      "Epoch 9/10\n",
      " - 8s - loss: 4.1102e-04 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 0.9778\n",
      "Epoch 10/10\n",
      " - 8s - loss: 3.1528e-04 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9778\n",
      "MACRO F1 :  0.9778812572759022\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.3722 - accuracy: 0.9219 - val_loss: 0.1078 - val_accuracy: 0.9813\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0451 - accuracy: 0.9920 - val_loss: 0.0681 - val_accuracy: 0.9801\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0091 - accuracy: 0.9994 - val_loss: 0.0650 - val_accuracy: 0.9790\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0033 - accuracy: 0.9999 - val_loss: 0.0663 - val_accuracy: 0.9790\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0668 - val_accuracy: 0.9778\n",
      "Epoch 6/10\n",
      " - 8s - loss: 8.9483e-04 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9778\n",
      "Epoch 7/10\n",
      " - 8s - loss: 5.5373e-04 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9778\n",
      "Epoch 8/10\n",
      " - 8s - loss: 3.7258e-04 - accuracy: 1.0000 - val_loss: 0.0726 - val_accuracy: 0.9778\n",
      "Epoch 9/10\n",
      " - 8s - loss: 2.6476e-04 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9778\n",
      "Epoch 10/10\n",
      " - 8s - loss: 1.9742e-04 - accuracy: 1.0000 - val_loss: 0.0752 - val_accuracy: 0.9778\n",
      "MACRO F1 :  0.9788182831661092\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4518 - accuracy: 0.8383 - val_loss: 0.1739 - val_accuracy: 0.9696\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0708 - accuracy: 0.9903 - val_loss: 0.0709 - val_accuracy: 0.9801\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0144 - accuracy: 0.9987 - val_loss: 0.0631 - val_accuracy: 0.9778\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0051 - accuracy: 0.9997 - val_loss: 0.0618 - val_accuracy: 0.9790\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.0619 - val_accuracy: 0.9790\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0616 - val_accuracy: 0.9801\n",
      "Epoch 7/10\n",
      " - 8s - loss: 9.4231e-04 - accuracy: 1.0000 - val_loss: 0.0633 - val_accuracy: 0.9778\n",
      "Epoch 8/10\n",
      " - 8s - loss: 6.7100e-04 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9778\n",
      "Epoch 9/10\n",
      " - 8s - loss: 4.9855e-04 - accuracy: 1.0000 - val_loss: 0.0649 - val_accuracy: 0.9778\n",
      "Epoch 10/10\n",
      " - 8s - loss: 3.8375e-04 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 0.9778\n",
      "MACRO F1 :  0.9789590254706535\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4130 - accuracy: 0.8560 - val_loss: 0.1507 - val_accuracy: 0.9661\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0540 - accuracy: 0.9914 - val_loss: 0.0846 - val_accuracy: 0.9731\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0109 - accuracy: 0.9987 - val_loss: 0.0803 - val_accuracy: 0.9720\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0039 - accuracy: 0.9999 - val_loss: 0.0793 - val_accuracy: 0.9743\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0793 - val_accuracy: 0.9720\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0809 - val_accuracy: 0.9720\n",
      "Epoch 7/10\n",
      " - 8s - loss: 7.0723e-04 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9708\n",
      "Epoch 8/10\n",
      " - 8s - loss: 5.0130e-04 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9708\n",
      "Epoch 9/10\n",
      " - 8s - loss: 3.7310e-04 - accuracy: 1.0000 - val_loss: 0.0838 - val_accuracy: 0.9708\n",
      "Epoch 10/10\n",
      " - 8s - loss: 2.8713e-04 - accuracy: 1.0000 - val_loss: 0.0849 - val_accuracy: 0.9696\n",
      "MACRO F1 :  0.969626168224299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.3716 - accuracy: 0.9190 - val_loss: 0.1245 - val_accuracy: 0.9708\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0446 - accuracy: 0.9917 - val_loss: 0.0844 - val_accuracy: 0.9708\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0089 - accuracy: 0.9992 - val_loss: 0.0810 - val_accuracy: 0.9708\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9696\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0845 - val_accuracy: 0.9708\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0880 - val_accuracy: 0.9720\n",
      "Epoch 7/10\n",
      " - 8s - loss: 7.4652e-04 - accuracy: 1.0000 - val_loss: 0.0894 - val_accuracy: 0.9708\n",
      "Epoch 8/10\n",
      " - 8s - loss: 5.3934e-04 - accuracy: 1.0000 - val_loss: 0.0904 - val_accuracy: 0.9708\n",
      "Epoch 9/10\n",
      " - 8s - loss: 4.0731e-04 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 0.9708\n",
      "Epoch 10/10\n",
      " - 8s - loss: 3.1729e-04 - accuracy: 1.0000 - val_loss: 0.0949 - val_accuracy: 0.9708\n",
      "MACRO F1 :  0.9708963911525029\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.3930 - accuracy: 0.9111 - val_loss: 0.1242 - val_accuracy: 0.9708\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0513 - accuracy: 0.9897 - val_loss: 0.0659 - val_accuracy: 0.9790\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0102 - accuracy: 0.9994 - val_loss: 0.0618 - val_accuracy: 0.9766\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0034 - accuracy: 0.9999 - val_loss: 0.0585 - val_accuracy: 0.9755\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 0.9755\n",
      "Epoch 6/10\n",
      " - 8s - loss: 7.8333e-04 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9755\n",
      "Epoch 7/10\n",
      " - 8s - loss: 4.8352e-04 - accuracy: 1.0000 - val_loss: 0.0609 - val_accuracy: 0.9766\n",
      "Epoch 8/10\n",
      " - 8s - loss: 3.2140e-04 - accuracy: 1.0000 - val_loss: 0.0615 - val_accuracy: 0.9766\n",
      "Epoch 9/10\n",
      " - 8s - loss: 2.2675e-04 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9766\n",
      "Epoch 10/10\n",
      " - 8s - loss: 1.6662e-04 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9766\n",
      "MACRO F1 :  0.9785867237687367\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "X_comb = vstack([xt,xv])\n",
    "y_comb = np.concatenate([np.array(yt),np.array(yv)])\n",
    "y_comb = np.where(y_comb==\"real\",1,0)\n",
    "X_comb = X_comb.tocsr()\n",
    "testX = testX.tocsr()\n",
    "X_comb, y_comb = shuffle(X_comb,y_comb)\n",
    "Xsplits=[]\n",
    "Ysplits=[]\n",
    "size = X_comb.shape[0]\n",
    "K=10\n",
    "sizeOfsplit = size//K\n",
    "for i in range(K):\n",
    "    Xsplits.append(X_comb[i*sizeOfsplit:(i+1)*sizeOfsplit,:])\n",
    "    Ysplits.append(y_comb[i*sizeOfsplit:(i+1)*sizeOfsplit])\n",
    "splits = [ tuple(filter(lambda x: x!=i,tuple(range(K)))) for i in range(K)]\n",
    "res = []\n",
    "testRes=[]\n",
    "for i in range(K):\n",
    "    Xtrain = vstack(tuple(Xsplits[x] for x in splits[i]))\n",
    "    ytrain = np.concatenate(tuple(Ysplits[x] for x in splits[i]))\n",
    "    \n",
    "    Xtest = Xsplits[i]\n",
    "    ytest = np.copy(Ysplits[i])\n",
    "    \n",
    "    with tf.device('/CPU:0'):\n",
    "        x_in = keras.layers.Input(shape=(xt.shape[1],))\n",
    "        x = Dense(64, activation='relu')(x_in)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         Dropout(0.2)\n",
    "        x = Dense(16, activation='relu')(x)  \n",
    "#         Dropout(0.2)\n",
    "#         x = BatchNormalization()(x)\n",
    "        y_out = Dense(1, activation='sigmoid')(x)\n",
    "        model = keras.models.Model(x_in,y_out)\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "#         print(model.summary())\n",
    "        training = model.fit(Xtrain,ytrain,batch_size=64,epochs=10, shuffle=\"batch\", verbose=2, validation_split=0,validation_data=(Xtest, ytest))\n",
    "        y_pred = model.predict(Xtest)\n",
    "        y_cut = np.where(y_pred>0.5,1,0)\n",
    "        macrof1 = f1_score(ytest, y_cut, zero_division=1)\n",
    "        print(\"MACRO F1 : \",macrof1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4031 - accuracy: 0.9181 - val_loss: 0.1479 - val_accuracy: 0.9650\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0529 - accuracy: 0.9904 - val_loss: 0.0827 - val_accuracy: 0.9731\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0109 - accuracy: 0.9990 - val_loss: 0.0741 - val_accuracy: 0.9743\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0039 - accuracy: 0.9999 - val_loss: 0.0725 - val_accuracy: 0.9743\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 0.9755\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0741 - val_accuracy: 0.9755\n",
      "Epoch 7/10\n",
      " - 8s - loss: 8.1116e-04 - accuracy: 1.0000 - val_loss: 0.0745 - val_accuracy: 0.9755\n",
      "Epoch 8/10\n",
      " - 8s - loss: 5.6010e-04 - accuracy: 1.0000 - val_loss: 0.0750 - val_accuracy: 0.9755\n",
      "Epoch 9/10\n",
      " - 8s - loss: 4.0072e-04 - accuracy: 1.0000 - val_loss: 0.0757 - val_accuracy: 0.9755\n",
      "Epoch 10/10\n",
      " - 8s - loss: 2.9706e-04 - accuracy: 1.0000 - val_loss: 0.0768 - val_accuracy: 0.9766\n",
      "MACRO F1 :  0.9778270509977828\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4036 - accuracy: 0.8869 - val_loss: 0.1448 - val_accuracy: 0.9755\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0547 - accuracy: 0.9922 - val_loss: 0.0734 - val_accuracy: 0.9836\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0107 - accuracy: 0.9991 - val_loss: 0.0632 - val_accuracy: 0.9825\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0040 - accuracy: 0.9999 - val_loss: 0.0608 - val_accuracy: 0.9848\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0608 - val_accuracy: 0.9836\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9825\n",
      "Epoch 7/10\n",
      " - 8s - loss: 9.1815e-04 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9836\n",
      "Epoch 8/10\n",
      " - 8s - loss: 6.6977e-04 - accuracy: 1.0000 - val_loss: 0.0617 - val_accuracy: 0.9836\n",
      "Epoch 9/10\n",
      " - 8s - loss: 5.0853e-04 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 0.9836\n",
      "Epoch 10/10\n",
      " - 8s - loss: 3.9768e-04 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 0.9836\n",
      "MACRO F1 :  0.9839080459770114\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.3796 - accuracy: 0.8615 - val_loss: 0.1198 - val_accuracy: 0.9731\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0457 - accuracy: 0.9931 - val_loss: 0.0679 - val_accuracy: 0.9790\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0087 - accuracy: 0.9991 - val_loss: 0.0618 - val_accuracy: 0.9790\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0032 - accuracy: 0.9999 - val_loss: 0.0615 - val_accuracy: 0.9790\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9801\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9801\n",
      "Epoch 7/10\n",
      " - 8s - loss: 6.6631e-04 - accuracy: 1.0000 - val_loss: 0.0629 - val_accuracy: 0.9801\n",
      "Epoch 8/10\n",
      " - 8s - loss: 4.6312e-04 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9801\n",
      "Epoch 9/10\n",
      " - 8s - loss: 3.3151e-04 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9801\n",
      "Epoch 10/10\n",
      " - 8s - loss: 2.4209e-04 - accuracy: 1.0000 - val_loss: 0.0649 - val_accuracy: 0.9801\n",
      "MACRO F1 :  0.9815016322089227\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4218 - accuracy: 0.9086 - val_loss: 0.1469 - val_accuracy: 0.9579\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0577 - accuracy: 0.9904 - val_loss: 0.0744 - val_accuracy: 0.9755\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0124 - accuracy: 0.9988 - val_loss: 0.0667 - val_accuracy: 0.9755\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0047 - accuracy: 0.9999 - val_loss: 0.0673 - val_accuracy: 0.9755\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9755\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0679 - val_accuracy: 0.9755\n",
      "Epoch 7/10\n",
      " - 8s - loss: 9.3368e-04 - accuracy: 1.0000 - val_loss: 0.0715 - val_accuracy: 0.9755\n",
      "Epoch 8/10\n",
      " - 8s - loss: 6.3640e-04 - accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 0.9755\n",
      "Epoch 9/10\n",
      " - 8s - loss: 4.4978e-04 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 0.9766\n",
      "Epoch 10/10\n",
      " - 8s - loss: 3.2975e-04 - accuracy: 1.0000 - val_loss: 0.0761 - val_accuracy: 0.9766\n",
      "MACRO F1 :  0.9774774774774775\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4025 - accuracy: 0.8985 - val_loss: 0.1400 - val_accuracy: 0.9650\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0510 - accuracy: 0.9914 - val_loss: 0.0875 - val_accuracy: 0.9650\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0105 - accuracy: 0.9992 - val_loss: 0.0798 - val_accuracy: 0.9673\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0041 - accuracy: 0.9997 - val_loss: 0.0797 - val_accuracy: 0.9673\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0810 - val_accuracy: 0.9661\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0804 - val_accuracy: 0.9673\n",
      "Epoch 7/10\n",
      " - 8s - loss: 8.2233e-04 - accuracy: 1.0000 - val_loss: 0.0812 - val_accuracy: 0.9673\n",
      "Epoch 8/10\n",
      " - 8s - loss: 5.8608e-04 - accuracy: 1.0000 - val_loss: 0.0817 - val_accuracy: 0.9673\n",
      "Epoch 9/10\n",
      " - 8s - loss: 4.3848e-04 - accuracy: 1.0000 - val_loss: 0.0829 - val_accuracy: 0.9661\n",
      "Epoch 10/10\n",
      " - 8s - loss: 3.3741e-04 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9661\n",
      "MACRO F1 :  0.9661610268378064\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4122 - accuracy: 0.9120 - val_loss: 0.1347 - val_accuracy: 0.9720\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0588 - accuracy: 0.9877 - val_loss: 0.0661 - val_accuracy: 0.9801\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0132 - accuracy: 0.9988 - val_loss: 0.0561 - val_accuracy: 0.9825\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0047 - accuracy: 0.9997 - val_loss: 0.0540 - val_accuracy: 0.9801\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9813\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 0.9836\n",
      "Epoch 7/10\n",
      " - 8s - loss: 9.3122e-04 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9825\n",
      "Epoch 8/10\n",
      " - 8s - loss: 6.4486e-04 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9825\n",
      "Epoch 9/10\n",
      " - 8s - loss: 4.5832e-04 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 0.9825\n",
      "Epoch 10/10\n",
      " - 8s - loss: 3.3263e-04 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 0.9825\n",
      "MACRO F1 :  0.9826989619377162\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4050 - accuracy: 0.9043 - val_loss: 0.1478 - val_accuracy: 0.9579\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0548 - accuracy: 0.9904 - val_loss: 0.0810 - val_accuracy: 0.9685\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0116 - accuracy: 0.9992 - val_loss: 0.0749 - val_accuracy: 0.9696\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0043 - accuracy: 0.9999 - val_loss: 0.0742 - val_accuracy: 0.9685\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0762 - val_accuracy: 0.9685\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 0.9685\n",
      "Epoch 7/10\n",
      " - 8s - loss: 9.3857e-04 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9696\n",
      "Epoch 8/10\n",
      " - 8s - loss: 6.8250e-04 - accuracy: 1.0000 - val_loss: 0.0782 - val_accuracy: 0.9696\n",
      "Epoch 9/10\n",
      " - 8s - loss: 5.1687e-04 - accuracy: 1.0000 - val_loss: 0.0793 - val_accuracy: 0.9696\n",
      "Epoch 10/10\n",
      " - 8s - loss: 4.0397e-04 - accuracy: 1.0000 - val_loss: 0.0799 - val_accuracy: 0.9696\n",
      "MACRO F1 :  0.9714912280701754\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4395 - accuracy: 0.9111 - val_loss: 0.1470 - val_accuracy: 0.9650\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0589 - accuracy: 0.9895 - val_loss: 0.0778 - val_accuracy: 0.9731\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0116 - accuracy: 0.9991 - val_loss: 0.0749 - val_accuracy: 0.9720\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0040 - accuracy: 0.9999 - val_loss: 0.0758 - val_accuracy: 0.9731\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.0756 - val_accuracy: 0.9731\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0771 - val_accuracy: 0.9731\n",
      "Epoch 7/10\n",
      " - 8s - loss: 7.3372e-04 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9731\n",
      "Epoch 8/10\n",
      " - 8s - loss: 5.1259e-04 - accuracy: 1.0000 - val_loss: 0.0806 - val_accuracy: 0.9731\n",
      "Epoch 9/10\n",
      " - 8s - loss: 3.7619e-04 - accuracy: 1.0000 - val_loss: 0.0819 - val_accuracy: 0.9731\n",
      "Epoch 10/10\n",
      " - 8s - loss: 2.8530e-04 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9731\n",
      "MACRO F1 :  0.9754535752401281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4112 - accuracy: 0.8677 - val_loss: 0.1472 - val_accuracy: 0.9626\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0543 - accuracy: 0.9912 - val_loss: 0.0869 - val_accuracy: 0.9778\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0108 - accuracy: 0.9995 - val_loss: 0.0790 - val_accuracy: 0.9755\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9755\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0800 - val_accuracy: 0.9755\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0818 - val_accuracy: 0.9766\n",
      "Epoch 7/10\n",
      " - 8s - loss: 7.5412e-04 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9766\n",
      "Epoch 8/10\n",
      " - 8s - loss: 4.7851e-04 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9766\n",
      "Epoch 9/10\n",
      " - 8s - loss: 3.1563e-04 - accuracy: 1.0000 - val_loss: 0.0876 - val_accuracy: 0.9755\n",
      "Epoch 10/10\n",
      " - 8s - loss: 2.1861e-04 - accuracy: 1.0000 - val_loss: 0.0893 - val_accuracy: 0.9766\n",
      "MACRO F1 :  0.9766899766899767\n",
      "Train on 7704 samples, validate on 856 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.4168 - accuracy: 0.8720 - val_loss: 0.1357 - val_accuracy: 0.9708\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0574 - accuracy: 0.9903 - val_loss: 0.0702 - val_accuracy: 0.9755\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0121 - accuracy: 0.9990 - val_loss: 0.0635 - val_accuracy: 0.9731\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0043 - accuracy: 0.9997 - val_loss: 0.0652 - val_accuracy: 0.9743\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9731\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0669 - val_accuracy: 0.9731\n",
      "Epoch 7/10\n",
      " - 8s - loss: 8.0527e-04 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9731\n",
      "Epoch 8/10\n",
      " - 8s - loss: 5.5165e-04 - accuracy: 1.0000 - val_loss: 0.0704 - val_accuracy: 0.9743\n",
      "Epoch 9/10\n",
      " - 8s - loss: 3.8898e-04 - accuracy: 1.0000 - val_loss: 0.0718 - val_accuracy: 0.9743\n",
      "Epoch 10/10\n",
      " - 8s - loss: 2.8016e-04 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9731\n",
      "MACRO F1 :  0.9753483386923902\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "X_comb = vstack([xt,xv])\n",
    "y_comb = np.concatenate([np.array(yt),np.array(yv)])\n",
    "y_comb = np.where(y_comb==\"real\",1,0)\n",
    "X_comb = X_comb.tocsr()\n",
    "testX = testX.tocsr()\n",
    "X_comb, y_comb = shuffle(X_comb,y_comb)\n",
    "Xsplits=[]\n",
    "Ysplits=[]\n",
    "size = X_comb.shape[0]\n",
    "K=10\n",
    "sizeOfsplit = size//K\n",
    "for i in range(K):\n",
    "    Xsplits.append(X_comb[i*sizeOfsplit:(i+1)*sizeOfsplit,:])\n",
    "    Ysplits.append(y_comb[i*sizeOfsplit:(i+1)*sizeOfsplit])\n",
    "splits = [ tuple(filter(lambda x: x!=i,tuple(range(K)))) for i in range(K)]\n",
    "res = []\n",
    "testRes=[]\n",
    "for i in range(K):\n",
    "    Xtrain = vstack(tuple(Xsplits[x] for x in splits[i]))\n",
    "    ytrain = np.concatenate(tuple(Ysplits[x] for x in splits[i]))\n",
    "    \n",
    "    Xtest = Xsplits[i]\n",
    "    ytest = np.copy(Ysplits[i])\n",
    "    \n",
    "    with tf.device('/CPU:0'):\n",
    "        x_in = keras.layers.Input(shape=(xt.shape[1],))\n",
    "        x = Dense(64, activation='relu')(x_in)\n",
    "#         x = BatchNormalization()(x)\n",
    "        Dropout(0.2)\n",
    "        x = Dense(16, activation='relu')(x)  \n",
    "        Dropout(0.2)\n",
    "#         x = BatchNormalization()(x)\n",
    "        y_out = Dense(1, activation='sigmoid')(x)\n",
    "        model = keras.models.Model(x_in,y_out)\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "#         print(model.summary())\n",
    "        training = model.fit(Xtrain,ytrain,batch_size=64,epochs=10, shuffle=\"batch\", verbose=2, validation_split=0,validation_data=(Xtest, ytest))\n",
    "        y_pred = model.predict(Xtest)\n",
    "        y_cut = np.where(y_pred>0.5,1,0)\n",
    "        macrof1 = f1_score(ytest, y_cut, zero_division=1)\n",
    "        print(\"MACRO F1 : \",macrof1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import GRU\n",
    "\n",
    "import keras.backend as kb\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = keras.layers.Permute((2,1))(inputs)\n",
    "    x = keras.layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = keras.layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = keras.layers.add([inputs, x])\n",
    "    return x\n",
    "def trainModelCNN(Xtrain,Xtrain_feat,embed,ytrain):\n",
    "    with tf.device('/CPU:0'):\n",
    "        x_in = keras.layers.Input(shape=(PADDING_SIZE,))\n",
    "        x_feat = keras.layers.Input(shape=(SENT_LEVEL_FEATURES+UNIGRAMS_CNT,))\n",
    "        unig = Dense(32, activation='relu')(x_feat)\n",
    "        unig = Dense(16, activation='relu')(unig)\n",
    "        \n",
    "        x = Embedding(input_dim=embed.shape[0],output_dim=embed.shape[1],weights=[embed],input_length=PADDING_SIZE,trainable=False)(x_in)\n",
    "        x1 = Conv1D(50,2, activation='relu',input_shape=(PADDING_SIZE,VECTOR_SIZE+EXTRA_VECTORS),padding='same') (x)\n",
    "        x1 = MaxPooling1D(pool_size=2) (x1)\n",
    "        x2 = Conv1D(50,3, activation='relu',input_shape=(PADDING_SIZE,VECTOR_SIZE+EXTRA_VECTORS),padding='same') (x)\n",
    "        x2 = MaxPooling1D(pool_size=2) (x2)\n",
    "        x3 = Conv1D(50,4, activation='relu',input_shape=(PADDING_SIZE,VECTOR_SIZE+EXTRA_VECTORS),padding='same') (x)\n",
    "        x3 = MaxPooling1D(pool_size=2) (x3)\n",
    "        x4 = Conv1D(50,5, activation='relu',input_shape=(PADDING_SIZE,VECTOR_SIZE+EXTRA_VECTORS),padding='same') (x)\n",
    "        x4 = MaxPooling1D(pool_size=2) (x4)\n",
    "        x = Concatenate(axis=2)([x1,x2,x3,x4])\n",
    "        x = Flatten()(x)\n",
    "#         x = attention_layer(x, neurons=PADDING_SIZE)\n",
    "#         x = Bidirectional(keras.layers.LSTM(units=50, dropout=0.2, return_sequences=True))(x)\n",
    "#         x = Bidirectional(keras.layers.LSTM(units=50, dropout=0.2))(x)\n",
    "        \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(10, activation='relu')(x)\n",
    "        \n",
    "        x = Concatenate(axis=1)([x]+[unig])\n",
    "        \n",
    "        y_out = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = keras.models.Model([x_in,x_feat], y_out)\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "\n",
    "        training = model.fit([Xtrain,Xtrain_feat],ytrain,batch_size=512,epochs=20, shuffle=True, verbose=1, validation_split=0)\n",
    "        \n",
    "        return training,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 40) (6420, 1)\n",
      "(2140, 40) (2140, 1)\n",
      "(2140, 40) (2140, 1)\n"
     ]
    }
   ],
   "source": [
    "test=pd.DataFrame(pd.read_csv(\"Constraint_English_Test - Sheet1.csv\"))\n",
    "train=pd.DataFrame(pd.read_csv(\"Constraint_English_Train - Sheet1.csv\"))\n",
    "val=pd.DataFrame(pd.read_csv(\"Constraint_English_Val - Sheet1.csv\"))\n",
    "\n",
    "processdf(test)\n",
    "processdf(train)\n",
    "processdf(val)\n",
    "\n",
    "vecmodel = pickle.loads(open('word2Vec','rb').read())\n",
    "Xlist,ylist,XFlist,embed = getEmbeddingsFeatures([train,val,test],vecmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8560, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_comb = np.vstack(Xlist[0:2])\n",
    "XF_comb = np.vstack(XFlist[0:2])\n",
    "y_comb = np.vstack(ylist[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "Xsplits=[]\n",
    "XFsplits=[]\n",
    "Ysplits=[]\n",
    "size = X_comb.shape[0]\n",
    "K=10\n",
    "sizeOfsplit = size//K\n",
    "for i in range(K):\n",
    "    Xsplits.append(X_comb[i*sizeOfsplit:(i+1)*sizeOfsplit,:])\n",
    "    XFsplits.append(XF_comb[i*sizeOfsplit:(i+1)*sizeOfsplit,:])\n",
    "    Ysplits.append(y_comb[i*sizeOfsplit:(i+1)*sizeOfsplit,:])\n",
    "splits = [ tuple(filter(lambda x: x!=i,tuple(range(K)))) for i in range(K)]\n",
    "res = []\n",
    "for i in range(K):\n",
    "    Xtrain = np.concatenate(tuple(Xsplits[x] for x in splits[i]))\n",
    "    XFtrain = np.concatenate(tuple(XFsplits[x] for x in splits[i]))\n",
    "    ytrain = np.concatenate(tuple(Ysplits[x] for x in splits[i]))\n",
    "    \n",
    "    Xtest = np.copy(Xsplits[i])\n",
    "    XFtest = np.copy(XFsplits[i])\n",
    "    ytest = np.copy(Ysplits[i])\n",
    "    \n",
    "    print(Xtrain.shape,XFtrain.shape,ytrain.shape)\n",
    "    \n",
    "    clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,tol=1e-3)\n",
    "    clf.fit(Xtrain,ytrain)\n",
    "    y_pred_pac=clf.predict(Xtest)\n",
    "    scoref1 = f1_score(y_pred_pac,ytest,average=\"macro\")\n",
    "    print(\"Fold : \"+str(i)+\" F1 Score\",scoref1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6848, 40) (6848, 4075) (6848, 1)\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 40, 203)      4982838     input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 40, 50)       20350       embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 40, 50)       30500       embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 40, 50)       40650       embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 40, 50)       50800       embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 20, 50)       0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 20, 50)       0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 20, 50)       0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 20, 50)       0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 20, 200)      0           max_pooling1d_53[0][0]           \n",
      "                                                                 max_pooling1d_54[0][0]           \n",
      "                                                                 max_pooling1d_55[0][0]           \n",
      "                                                                 max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4000)         0           concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 4075)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 32)           128032      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 32)           130432      input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 10)           330         dense_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 16)           528         dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 26)           0           dense_60[0][0]                   \n",
      "                                                                 dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 1)            27          concatenate_23[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 5,384,487\n",
      "Trainable params: 401,649\n",
      "Non-trainable params: 4,982,838\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "6848/6848 [==============================] - 12s 2ms/step - loss: 3.1609 - accuracy: 0.5524\n",
      "Epoch 2/20\n",
      "6848/6848 [==============================] - 8s 1ms/step - loss: 0.8771 - accuracy: 0.6495\n",
      "Epoch 3/20\n",
      "6848/6848 [==============================] - 11s 2ms/step - loss: 0.4736 - accuracy: 0.8011\n",
      "Epoch 4/20\n",
      "6848/6848 [==============================] - 13s 2ms/step - loss: 0.3195 - accuracy: 0.8655\n",
      "Epoch 5/20\n",
      "6848/6848 [==============================] - 11s 2ms/step - loss: 0.2532 - accuracy: 0.9045\n",
      "Epoch 6/20\n",
      "6848/6848 [==============================] - 10s 1ms/step - loss: 0.2156 - accuracy: 0.9194\n",
      "Epoch 7/20\n",
      "6848/6848 [==============================] - 10s 1ms/step - loss: 0.1890 - accuracy: 0.9315\n",
      "Epoch 8/20\n",
      "6848/6848 [==============================] - 12s 2ms/step - loss: 0.1717 - accuracy: 0.9404\n",
      "Epoch 9/20\n",
      "6848/6848 [==============================] - 9s 1ms/step - loss: 0.1566 - accuracy: 0.9414\n",
      "Epoch 10/20\n",
      "6848/6848 [==============================] - 8s 1ms/step - loss: 0.1398 - accuracy: 0.9533\n",
      "Epoch 11/20\n",
      "6848/6848 [==============================] - 8s 1ms/step - loss: 0.1272 - accuracy: 0.9572\n",
      "Epoch 12/20\n",
      "6848/6848 [==============================] - 9s 1ms/step - loss: 0.1136 - accuracy: 0.9626\n",
      "Epoch 13/20\n",
      "6848/6848 [==============================] - 9s 1ms/step - loss: 0.0993 - accuracy: 0.9690\n",
      "Epoch 14/20\n",
      "6848/6848 [==============================] - 10s 1ms/step - loss: 0.0864 - accuracy: 0.9752\n",
      "Epoch 15/20\n",
      "6848/6848 [==============================] - 9s 1ms/step - loss: 0.0630 - accuracy: 0.9841\n",
      "Epoch 16/20\n",
      "6848/6848 [==============================] - 10s 1ms/step - loss: 0.0473 - accuracy: 0.9899\n",
      "Epoch 17/20\n",
      "6848/6848 [==============================] - 8s 1ms/step - loss: 0.0319 - accuracy: 0.9940\n",
      "Epoch 18/20\n",
      "6848/6848 [==============================] - 8s 1ms/step - loss: 0.0202 - accuracy: 0.9978\n",
      "Epoch 19/20\n",
      "6848/6848 [==============================] - 8s 1ms/step - loss: 0.0131 - accuracy: 0.9988\n",
      "Epoch 20/20\n",
      "6848/6848 [==============================] - 9s 1ms/step - loss: 0.0089 - accuracy: 0.9993\n",
      "Accuracy: 91.58878504672897\n",
      "Fold : 0 F1 Score 0.9158878504672897\n",
      "(6848, 40) (6848, 4075) (6848, 1)\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 40, 203)      4982838     input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 40, 50)       20350       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 40, 50)       30500       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 40, 50)       40650       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 40, 50)       50800       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 20, 50)       0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 20, 50)       0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 20, 50)       0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 20, 50)       0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 20, 200)      0           max_pooling1d_57[0][0]           \n",
      "                                                                 max_pooling1d_58[0][0]           \n",
      "                                                                 max_pooling1d_59[0][0]           \n",
      "                                                                 max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4000)         0           concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, 4075)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 32)           128032      flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 32)           130432      input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 10)           330         dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 16)           528         dense_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 26)           0           dense_65[0][0]                   \n",
      "                                                                 dense_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 1)            27          concatenate_25[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 5,384,487\n",
      "Trainable params: 401,649\n",
      "Non-trainable params: 4,982,838\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6848/6848 [==============================] - 12s 2ms/step - loss: 1.3229 - accuracy: 0.5799\n",
      "Epoch 2/20\n",
      "6848/6848 [==============================] - 11s 2ms/step - loss: 0.5604 - accuracy: 0.7373\n",
      "Epoch 3/20\n",
      "2048/6848 [=======>......................] - ETA: 5s - loss: 0.4409 - accuracy: 0.8218"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-9938cb3287d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXFtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainModelCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXFtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXFtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mmacrof1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetF1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetConfusion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-ea593ad305ad>\u001b[0m in \u001b[0;36mtrainModelCNN\u001b[1;34m(Xtrain, Xtrain_feat, embed, ytrain)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXtrain_feat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ker\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ker\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ker\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ker\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ker\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ker\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ker\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ker\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Xsplits=[]\n",
    "XFsplits=[]\n",
    "Ysplits=[]\n",
    "size = X_comb.shape[0]\n",
    "K=5\n",
    "sizeOfsplit = size//K\n",
    "for i in range(K):\n",
    "    Xsplits.append(X_comb[i*sizeOfsplit:(i+1)*sizeOfsplit,:])\n",
    "    XFsplits.append(XF_comb[i*sizeOfsplit:(i+1)*sizeOfsplit,:])\n",
    "    Ysplits.append(y_comb[i*sizeOfsplit:(i+1)*sizeOfsplit,:])\n",
    "splits = [ tuple(filter(lambda x: x!=i,tuple(range(K)))) for i in range(K)]\n",
    "res = []\n",
    "for i in range(K):\n",
    "    Xtrain = np.concatenate(tuple(Xsplits[x] for x in splits[i]))\n",
    "    XFtrain = np.concatenate(tuple(XFsplits[x] for x in splits[i]))\n",
    "    ytrain = np.concatenate(tuple(Ysplits[x] for x in splits[i]))\n",
    "    \n",
    "    Xtest = np.copy(Xsplits[i])\n",
    "    XFtest = np.copy(XFsplits[i])\n",
    "    ytest = np.copy(Ysplits[i])\n",
    "    \n",
    "    print(Xtrain.shape,XFtrain.shape,ytrain.shape)\n",
    "    \n",
    "    training,model = trainModelCNN(Xtrain,XFtrain,embed,ytrain)\n",
    "    y_pred = model.predict([Xtest,XFtest])\n",
    "    macrof1 = getF1(getConfusion(ytest,y_pred,2,cutoff=0.8))[0][2]\n",
    "    res.append((model,macrof1))\n",
    "    print(\"Fold : \"+str(i)+\" F1 Score\",macrof1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getLexicon1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-bbb19aa768e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mvecmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2Vec'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mXlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mylist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXFlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetEmbeddingsFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvecmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainModelCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXFlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mylist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-0fb8896a9791>\u001b[0m in \u001b[0;36mgetEmbeddingsFeatures\u001b[1;34m(corpusList, vecmodel)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# print(tk.word_index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# LEXICON FEATURES APPEND\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mdic1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetLexicon1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mdic2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetLexicon2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mdic3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetLexicon3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'getLexicon1' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "test=pd.DataFrame(pd.read_csv(\"Constraint_English_Test - Sheet1.csv\"))\n",
    "train=pd.DataFrame(pd.read_csv(\"Constraint_English_Train - Sheet1.csv\"))\n",
    "val=pd.DataFrame(pd.read_csv(\"Constraint_English_Val - Sheet1.csv\"))\n",
    "\n",
    "processdf(test)\n",
    "processdf(train)\n",
    "processdf(val)\n",
    "\n",
    "# vecmodel = saveWord2VecModel([train,val,test])\n",
    "\n",
    "vecmodel = pickle.loads(open('word2Vec','rb').read())\n",
    "Xlist,ylist,XFlist,embed = getEmbeddingsFeatures([train,val,test],vecmodel)\n",
    "\n",
    "training,model = trainModelCNN(Xlist[0],XFlist[0],embed,ylist[0])\n",
    "\n",
    "y_pred = model.predict([Xlist[1],XFlist[1]])\n",
    "yPred = np.where(y_pred > 0.5, 1, 0)\n",
    "func_Eval(ylist[1].flatten(),yPred.flatten())\n",
    "print(\"F1 Score\",getF1(getConfusion(ylist[1],y_pred,2,cutoff=0.8))[0][2])\n",
    "# for i in range(1,100):\n",
    "#     cutoff = (1/100) * i\n",
    "#     print(\"F1 Score\",getF1(getConfusion(ylist[1],y_pred,2,cutoff=cutoff))[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([Xlist[2],XFlist[2]])\n",
    "yPred = np.where(y_pred > 0.8, 1, 0)\n",
    "f = open(\"answer.txt\", \"a\")\n",
    "f.write(\"id,label\\n\")\n",
    "c=1\n",
    "for i in range(len(yPred.flatten())):\n",
    "    if(yPred[i]==0):\n",
    "        f.write(str(i+1)+\",\"+\"fake\")\n",
    "    else:\n",
    "        f.write(str(i+1)+\",\"+\"real\")\n",
    "    if(i!=len(yPred.flatten())-1):\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[our, daily, update, is, published, state, rep...</td>\n",
       "      <td>0</td>\n",
       "      <td>[36, 97, 73, 10, 170, 23, 49, 2, 72, 21, 2, 72...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[alfalfa, is, the, only, cure, for, covidz]</td>\n",
       "      <td>0</td>\n",
       "      <td>[21455, 10, 3, 99, 158, 12, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[president, trump, asked, what, he, would, do,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[139, 93, 765, 213, 108, 230, 114, 66, 108, 61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[state, reported, X, death, we, are, still, se...</td>\n",
       "      <td>0</td>\n",
       "      <td>[23, 49, 2, 27, 25, 14, 191, 1109, 6, 2204, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[this, is, the, sixth, time, a, global, health...</td>\n",
       "      <td>0</td>\n",
       "      <td>[29, 10, 3, 3169, 87, 6, 416, 57, 467, 22, 38,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>2136</td>\n",
       "      <td>[coronavirusupdates, statewise, detail, of, to...</td>\n",
       "      <td>0</td>\n",
       "      <td>[128, 1225, 161, 4, 40, 45, 8, 11, 545, 2, 215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>2137</td>\n",
       "      <td>[tonight, X, midnight, onwards, disaster, mana...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2010, 2, 4094, 24623, 1248, 623, 506, 22, 38,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>2138</td>\n",
       "      <td>[X, new, case, of, covidznigeria, plateau, X, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 18, 11, 4, 258, 295, 2, 361, 2, 224, 2, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>2139</td>\n",
       "      <td>[rt, cdcemergency, dyk, cdcgovs, onestop, shop...</td>\n",
       "      <td>0</td>\n",
       "      <td>[82, 1442, 1405, 24625, 6137, 1244, 12, 8, 983...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>2140</td>\n",
       "      <td>[more, than, half, of, pregnant, woman, recent...</td>\n",
       "      <td>0</td>\n",
       "      <td>[30, 50, 733, 4, 1409, 287, 797, 1814, 5, 6, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2140 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  label  \\\n",
       "0        1  [our, daily, update, is, published, state, rep...      0   \n",
       "1        2        [alfalfa, is, the, only, cure, for, covidz]      0   \n",
       "2        3  [president, trump, asked, what, he, would, do,...      0   \n",
       "3        4  [state, reported, X, death, we, are, still, se...      0   \n",
       "4        5  [this, is, the, sixth, time, a, global, health...      0   \n",
       "...    ...                                                ...    ...   \n",
       "2135  2136  [coronavirusupdates, statewise, detail, of, to...      0   \n",
       "2136  2137  [tonight, X, midnight, onwards, disaster, mana...      0   \n",
       "2137  2138  [X, new, case, of, covidznigeria, plateau, X, ...      0   \n",
       "2138  2139  [rt, cdcemergency, dyk, cdcgovs, onestop, shop...      0   \n",
       "2139  2140  [more, than, half, of, pregnant, woman, recent...      0   \n",
       "\n",
       "                                               tweet_id  \n",
       "0     [36, 97, 73, 10, 170, 23, 49, 2, 72, 21, 2, 72...  \n",
       "1                        [21455, 10, 3, 99, 158, 12, 8]  \n",
       "2     [139, 93, 765, 213, 108, 230, 114, 66, 108, 61...  \n",
       "3     [23, 49, 2, 27, 25, 14, 191, 1109, 6, 2204, 20...  \n",
       "4     [29, 10, 3, 3169, 87, 6, 416, 57, 467, 22, 38,...  \n",
       "...                                                 ...  \n",
       "2135  [128, 1225, 161, 4, 40, 45, 8, 11, 545, 2, 215...  \n",
       "2136  [2010, 2, 4094, 24623, 1248, 623, 506, 22, 38,...  \n",
       "2137  [2, 18, 11, 4, 258, 295, 2, 361, 2, 224, 2, 14...  \n",
       "2138  [82, 1442, 1405, 24625, 6137, 1244, 12, 8, 983...  \n",
       "2139  [30, 50, 733, 4, 1409, 287, 797, 1814, 5, 6, 2...  \n",
       "\n",
       "[2140 rows x 4 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                      | 0/10700 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\confusement\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████| 10700/10700 [00:20<00:00, 528.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>decoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the cdc currently reports X deaths in general ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 1996, 26629, 2747, 4311, 1060, 6677, 199...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] the cdc currently reports x deaths in ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>states reported X deaths a small rise from las...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 2163, 2988, 1060, 6677, 1037, 2235, 4125...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] states reported x deaths a small rise fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>politically correct woman almost uses pandemic...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[101, 10317, 6149, 2450, 2471, 3594, 6090, 320...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] politically correct woman almost uses pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>indiafightscorona we have X covid testing labo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 2634, 20450, 9363, 26788, 2057, 2031, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] indiafightscorona we have x covid testin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>populous states can generate large case counts...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 20151, 2163, 2064, 9699, 2312, 2553, 929...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] populous states can generate large case ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>2136</td>\n",
       "      <td>donald trump wrongly claimed that new zealand ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[101, 6221, 8398, 29116, 3555, 2008, 2047, 341...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] donald trump wrongly claimed that new ze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>2137</td>\n",
       "      <td>current understanding is covidz spreads mostly...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 2783, 4824, 2003, 2522, 17258, 2480, 208...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] current understanding is covidz spreads ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>2138</td>\n",
       "      <td>nothing screams i am sat around doing fuck all...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[101, 2498, 11652, 1045, 2572, 2938, 2105, 272...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] nothing screams i am sat around doing fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>2139</td>\n",
       "      <td>birx says covidz outbreak not under control be...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[101, 12170, 2099, 2595, 2758, 2522, 17258, 24...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] birx says covidz outbreak not under cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>2140</td>\n",
       "      <td>another X new coronavirus cases have been conf...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[101, 2178, 1060, 2047, 21887, 23350, 3572, 20...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[CLS] another x new coronavirus cases have bee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10700 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  label  \\\n",
       "0        1  the cdc currently reports X deaths in general ...    1.0   \n",
       "1        2  states reported X deaths a small rise from las...    1.0   \n",
       "2        3  politically correct woman almost uses pandemic...    0.0   \n",
       "3        4  indiafightscorona we have X covid testing labo...    1.0   \n",
       "4        5  populous states can generate large case counts...    1.0   \n",
       "...    ...                                                ...    ...   \n",
       "2135  2136  donald trump wrongly claimed that new zealand ...    0.0   \n",
       "2136  2137  current understanding is covidz spreads mostly...    1.0   \n",
       "2137  2138  nothing screams i am sat around doing fuck all...    0.0   \n",
       "2138  2139  birx says covidz outbreak not under control be...    0.0   \n",
       "2139  2140  another X new coronavirus cases have been conf...    1.0   \n",
       "\n",
       "                                              input_ids  \\\n",
       "0     [101, 1996, 26629, 2747, 4311, 1060, 6677, 199...   \n",
       "1     [101, 2163, 2988, 1060, 6677, 1037, 2235, 4125...   \n",
       "2     [101, 10317, 6149, 2450, 2471, 3594, 6090, 320...   \n",
       "3     [101, 2634, 20450, 9363, 26788, 2057, 2031, 10...   \n",
       "4     [101, 20151, 2163, 2064, 9699, 2312, 2553, 929...   \n",
       "...                                                 ...   \n",
       "2135  [101, 6221, 8398, 29116, 3555, 2008, 2047, 341...   \n",
       "2136  [101, 2783, 4824, 2003, 2522, 17258, 2480, 208...   \n",
       "2137  [101, 2498, 11652, 1045, 2572, 2938, 2105, 272...   \n",
       "2138  [101, 12170, 2099, 2595, 2758, 2522, 17258, 24...   \n",
       "2139  [101, 2178, 1060, 2047, 21887, 23350, 3572, 20...   \n",
       "\n",
       "                                            segment_ids  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2135  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2136  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2137  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2138  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2139  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         attention_mask  \\\n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "2135  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2136  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2137  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2138  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2139  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                decoded  \n",
       "0     [CLS] the cdc currently reports x deaths in ge...  \n",
       "1     [CLS] states reported x deaths a small rise fr...  \n",
       "2     [CLS] politically correct woman almost uses pa...  \n",
       "3     [CLS] indiafightscorona we have x covid testin...  \n",
       "4     [CLS] populous states can generate large case ...  \n",
       "...                                                 ...  \n",
       "2135  [CLS] donald trump wrongly claimed that new ze...  \n",
       "2136  [CLS] current understanding is covidz spreads ...  \n",
       "2137  [CLS] nothing screams i am sat around doing fu...  \n",
       "2138  [CLS] birx says covidz outbreak not under cont...  \n",
       "2139  [CLS] another x new coronavirus cases have bee...  \n",
       "\n",
       "[10700 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PADDING_SIZE = 30\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "corpus = pd.concat([train,test,val])\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "segment_ids = []\n",
    "decoded=[]\n",
    "for i in tqdm(range(len(corpus))):\n",
    "    bert_inp= bert_tokenizer.encode_plus(corpus.iloc[i]['tweet'],add_special_tokens=True,max_length =PADDING_SIZE,pad_to_max_length = True,return_attention_mask=True)\n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "    decoded.append(bert_tokenizer.decode(bert_inp['input_ids']))\n",
    "    segment_ids.append([0]*PADDING_SIZE)\n",
    "corpus['input_ids'] = input_ids\n",
    "corpus['segment_ids'] = segment_ids\n",
    "corpus['attention_mask'] = attention_masks\n",
    "corpus['decoded'] = decoded\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 30) (8560, 30) (8560, 30)\n",
      "(6420, 30) (8560, 30) (8560, 30)\n",
      "(6420, 30) (8560, 30) (8560, 30)\n",
      "(6420,) (2140,)\n"
     ]
    }
   ],
   "source": [
    "train1 = corpus.iloc[:6420]\n",
    "test1 = corpus.iloc[:8560]\n",
    "val1 = corpus.iloc[8560:]\n",
    "\n",
    "Xtrain_id = np.vstack(train1['input_ids'])\n",
    "Xtrain_att = np.vstack(train1['attention_mask'])\n",
    "Xtrain_seg = np.vstack(train1['segment_ids'])\n",
    "\n",
    "Xtest_id = np.vstack(test1['input_ids'])\n",
    "Xtest_att = np.vstack(test1['attention_mask'])\n",
    "Xtest_seg = np.vstack(test1['segment_ids'])\n",
    "\n",
    "Xval_id = np.vstack(test1['input_ids'])\n",
    "Xval_att = np.vstack(test1['attention_mask'])\n",
    "Xval_seg = np.vstack(test1['segment_ids'])\n",
    "\n",
    "trainComb = (Xtrain_id,Xtrain_att,Xtrain_seg)\n",
    "valComb = (Xval_id,Xval_att,Xval_seg)\n",
    "\n",
    "ytrain = np.array(train1['label'])\n",
    "yval = np.array(val1['label'])\n",
    "\n",
    "print(Xtrain_id.shape,Xtest_id.shape,Xval_id.shape)\n",
    "print(Xtrain_att.shape,Xtest_att.shape,Xval_att.shape)\n",
    "print(Xtrain_seg.shape,Xtest_seg.shape,Xval_seg.shape)\n",
    "\n",
    "print(ytrain.shape,yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "\n",
    "#MODEL 2\n",
    "input_word_ids = tf.keras.Input(shape=(PADDING_SIZE,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = tf.keras.Input(shape=(PADDING_SIZE,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.keras.Input(shape=(PADDING_SIZE,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "clf_output = sequence_output[:, 0, :]\n",
    "net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
    "net = tf.keras.layers.Dropout(0.2)(net)\n",
    "# net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "# net = tf.keras.layers.Dropout(0.2)(net)\n",
    "out = tf.keras.layers.Dense(1, activation='softmax')(net)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#FIT\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "train_history = model.fit(trainComb,ytrain, validation_split=0.2,epochs=1,callbacks=[checkpoint, earlystopping],batch_size=256,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.7423 - binary_accuracy: 0.4961\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.7423 - binary_accuracy: 0.4961\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 7.7423 - binary_accuracy: 0.4961\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 7.7423 - binary_accuracy: 0.4961\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.7423 - binary_accuracy: 0.4961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2f0010d7220>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(10, input_shape=(30,)))\n",
    "model.add(keras.layers.Dense(1, input_shape=(30,)))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "model.fit(x=Xtrain_id,y=ytrain,shuffle=True,epochs=5,batch_size=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
