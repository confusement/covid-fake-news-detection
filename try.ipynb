{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_caller(Tweets_List):\n",
    "    stop_words_list=stopwords.words('english')\n",
    "    for i in (range(len(Tweets_List))):\n",
    "        Tweets_List[i]=preprocess_text(Tweets_List[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tweet, flag_stemm=False, flag_lemm=False, stop_words_list=None):\n",
    "#     print(tweet)\n",
    "    tweet=emoji.demojize(tweet)\n",
    "    tweet=tweet.replace(\":\",\" \")\n",
    "#     print(tweet)\n",
    "\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', str(tweet).lower().strip())\n",
    "    ## Tokenize (convert from string to list) and remove the stop words\n",
    "#     print(tweet)\n",
    "\n",
    "    tokenize_tweet = tweet.split()\n",
    "\n",
    "    if stop_words_list is not None:\n",
    "        tokenize_tweet = [word for word in tokenize_tweet if word not in stop_words_list]\n",
    "\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flag_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        tokenize_tweet = [ps.stem(word) for word in tokenize_tweet]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flag_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        tokenize_tweet = [lem.lemmatize(word) for word in tokenize_tweet]\n",
    "#     print(tokenize_tweet)\n",
    "    ## back to string from list\n",
    "    tweet = \" \".join(tokenize_tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=pd.DataFrame(pd.read_csv(\"Constraint_English_Train - Sheet1.csv\"))\n",
    "df_training.drop(columns=\"id\",inplace=True)\n",
    "df_training\n",
    "df_training['tweet_original']=df_training['tweet']\n",
    "preprocess_text_caller(df_training['tweet'])\n",
    "\n",
    "\n",
    "\n",
    "# df_testing=pd.DataFrame(pd.read_csv(\"testset-levela.tsv\",sep=\"\\t\"))\n",
    "# df_testing['tweet_original']=df_testing['tweet']\n",
    "# preprocess_text_caller(df_testing['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# ngram_range parameter (1,2) means that unigram and bigram will be taken\n",
    "# Count Vecotrizer automatically preprocess the tweets\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4),min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_ngrams=vectorizer.fit_transform(df_training.tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d990336c493d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_ngrams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation=pd.DataFrame(pd.read_csv(\"Constraint_English_Val - Sheet1.csv\"))\n",
    "df_validation.drop(columns=\"id\",inplace=True)\n",
    "df_validation\n",
    "df_validation['tweet_original']=df_validation['tweet']\n",
    "preprocess_text_caller(df_validation['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_ngrams=vectorizer.transform(df_validation.tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2140x3250 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 59317 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Validation_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred=clf.predict(Validation_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9096900112149515"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9098130841121496"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "clf.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred=clf.predict(Validation_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
