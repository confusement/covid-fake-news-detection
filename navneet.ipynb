{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "from sklearn.metrics import *\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_Eval(Y_Actual,Y_pred):\n",
    "\n",
    "    TrueNegative=0\n",
    "    TruePositive=0\n",
    "    FalsePositive=0\n",
    "    FalseNegative=0\n",
    "    for i in range(len(Y_pred)):\n",
    "        #if the actual class is negative\n",
    "        if(Y_Actual[i]==\"fake\"):\n",
    "            \n",
    "            #if the predicited class is negative\n",
    "            if(Y_pred[i]==\"fake\"):\n",
    "                TrueNegative+=1\n",
    "                \n",
    "            #if the predicted class is positive\n",
    "            else:\n",
    "                FalsePositive+=1\n",
    "                \n",
    "        #if the actual class is positive\n",
    "        else:\n",
    "            #if the predicited class is positive\n",
    "            if(Y_pred[i]==\"real\"):\n",
    "                TruePositive+=1\n",
    "                \n",
    "            #if the predicited class is negative\n",
    "            else:\n",
    "                FalseNegative+=1\n",
    "\n",
    "    Confusion_Matrix=[[TrueNegative,FalsePositive],[FalseNegative,TruePositive]]\n",
    "    \n",
    "    Confusion_Matrix=pd.DataFrame(Confusion_Matrix,columns=['Actual Negative','Actual Positive'])\n",
    "    \n",
    "    Confusion_Matrix.rename(index={0: \"Predicted Negative\", 1: \"Predicted Positive\"},inplace=True)\n",
    "    \n",
    "    MyPrecision=TruePositive/(TruePositive+FalsePositive)\n",
    "\n",
    "    MyRecall=TruePositive/(TruePositive+FalseNegative)\n",
    "    \n",
    "    MyAccuracy=(TruePositive+TrueNegative)/(TruePositive+TrueNegative+FalseNegative+FalsePositive)\n",
    "    \n",
    "    MyF1score= 2*(MyPrecision*MyRecall)/(MyPrecision+MyRecall)\n",
    "    \n",
    "    MyPrecisionZero=TrueNegative/(TrueNegative+FalseNegative)\n",
    "\n",
    "    MyRecallZero=TrueNegative/(TrueNegative+FalsePositive)\n",
    "    \n",
    "    \n",
    "    MyF1scoreZero= 2*(MyPrecisionZero*MyRecallZero)/(MyPrecisionZero+MyRecallZero)\n",
    "\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "    print(\"Accuracy:\",MyAccuracy)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Positive CLass, Label:4\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecision)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecall)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1score)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Negative CLass, Label:0\")\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecisionZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecallZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1scoreZero)\n",
    "\n",
    "    print()\n",
    "    print(\"--------\")\n",
    "    print(\"Macro Average F1\",(MyF1score+MyF1scoreZero)/2)\n",
    "    print(\"-----------------\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(Confusion_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_caller(Tweets_List):\n",
    "    stop_words_list=stopwords.words('english')\n",
    "    for i in (range(len(Tweets_List))):\n",
    "        Tweets_List[i]=preprocess_text(Tweets_List[i],stop_words_list=stop_words_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tweet, flag_stemm=True, flag_lemm=True, stop_words_list=None):\n",
    "#     print(tweet)\n",
    "    tweet=emoji.demojize(tweet)\n",
    "    tweet=tweet.replace(\":\",\" \")\n",
    "#     print(tweet)\n",
    "#     tweet=p.clean(tweet)\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip and remove url)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', str(tweet).lower().strip())\n",
    "#     tweet = re.sub(r'19', 'Z', str(tweet).lower().strip())\n",
    "#     tweet = re.sub(r'[0-9]+k*', ' X ', str(tweet).lower().strip())    \n",
    "#     tweet = re.sub(r'[^\\w\\s]', '', str(tweet).lower().strip())\n",
    "    ## Tokenize (convert from string to list) and remove the stop words\n",
    "#     print(tweet)\n",
    "\n",
    "    tokenize_tweet = tweet.split()\n",
    "\n",
    "    if stop_words_list is not None:\n",
    "        tokenize_tweet = [word for word in tokenize_tweet if word not in stop_words_list]\n",
    "\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flag_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        tokenize_tweet = [ps.stem(word) for word in tokenize_tweet]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flag_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        tokenize_tweet = [lem.lemmatize(word) for word in tokenize_tweet]\n",
    "#     print(tokenize_tweet)\n",
    "    ## back to string from list\n",
    "    tweet = \" \".join(tokenize_tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLength(GivenTweets):\n",
    "    ret=np.zeros(len(GivenTweets))\n",
    "    for i in (range(len(GivenTweets))):\n",
    "        tweet=GivenTweets[i]\n",
    "        ret[i] = len(tweet)\n",
    "    return ret\n",
    "def getAvgWord(GivenTweets):\n",
    "    \n",
    "    ret=np.zeros(len(GivenTweets))\n",
    "    for i in (range(len(GivenTweets))):\n",
    "        tweet=GivenTweets[i].split()\n",
    "        cnt = 0\n",
    "        add = 0\n",
    "        for j in range(len(tweet)):\n",
    "            cnt+=1\n",
    "            add+= len(tweet[j])\n",
    "        ret[i] = add/cnt\n",
    "    return ret\n",
    "def getSpecial(GivenTweets):\n",
    "    ret=np.zeros(len(GivenTweets))\n",
    "    for i in (range(len(GivenTweets))):\n",
    "        tweet=GivenTweets[i].split()\n",
    "        cnt = 0\n",
    "        for j in range(len(tweet)):\n",
    "            if(tweet[j]=='X'):\n",
    "                cnt+=1\n",
    "        ret[i] = cnt\n",
    "    return ret\n",
    "    \n",
    "def vader( GivenTweets):\n",
    "    obj = SentimentIntensityAnalyzer()\n",
    "    Comp_vader = np.zeros(len(GivenTweets))\n",
    "    for i in range(len(GivenTweets)):\n",
    "        sentiment_dict = obj.polarity_scores(GivenTweets[i])\n",
    "        Comp_vader[i]=sentiment_dict['compound']\n",
    "    return(Comp_vader)\n",
    "def FuncAllCapsCount( GivenTweets):\n",
    "    # Function to get All caps count from the tweets\n",
    "    # pass tweets as it is\n",
    "\n",
    "    AllCapsCount = np.zeros(len(GivenTweets))\n",
    "    for i in range(len(GivenTweets)):\n",
    "        tweet = nltk.word_tokenize(GivenTweets[i])\n",
    "        for word in tweet:\n",
    "            if(word!=\"I\" and word != \"USER\" and word != \"URL\" and re.match(\"^[A-Z]+$\",word)):\n",
    "                AllCapsCount[i] += 1\n",
    "\n",
    "    return AllCapsCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=pd.DataFrame(pd.read_csv(\"Constraint_English_Train - Sheet1.csv\"))\n",
    "df_training.drop(columns=\"id\",inplace=True)\n",
    "# df_training\n",
    "df_training['tweet_original']=df_training['tweet']\n",
    "preprocess_text_caller(df_training['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation=pd.DataFrame(pd.read_csv(\"Constraint_English_Val - Sheet1.csv\"))\n",
    "df_validation.drop(columns=\"id\",inplace=True)\n",
    "# df_validation\n",
    "df_validation['tweet_original']=df_validation['tweet']\n",
    "preprocess_text_caller(df_validation['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-559-fb9ae317fd00>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Tweets_List[i]=preprocess_text(Tweets_List[i],stop_words_list=stop_words_list)\n"
     ]
    }
   ],
   "source": [
    "df_test=pd.DataFrame(pd.read_csv(\"Constraint_English_Test - Sheet1.csv\"))\n",
    "# df_test\n",
    "df_test['tweet_original']=df_test['tweet']\n",
    "preprocess_text_caller(df_test['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_ngrams=vectorizer.fit_transform(df_training.tweet)\n",
    "vectorizer.fit_transform(list(df_training.tweet) + list(df_validation.tweet) +list(df_test.tweet))\n",
    "Train_ngrams=vectorizer.transform(df_training.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_f=pd.DataFrame(FuncAllCapsCount(df_validation.tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_ngrams=vectorizer.transform(df_validation.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2140x28361 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50472 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Validation_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_ngrams=vectorizer.transform(df_test.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_rf=clf.predict(Validation_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9214293769160768"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_rf,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(y_pred,df_validation[\"label\"],average=\"macro\")\n",
    "# func_Eval(y_pred,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\"))\n",
    "clf.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_svm=clf.predict(Validation_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420009074925841"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_svm,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_Eval(y_pred_svm,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_log=clf.predict(Validation_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9199220797238565"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_log,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_Eval(y_pred_log,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9302788169483306"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf2 = MLPClassifier(random_state=1, max_iter=300,verbose=True,activation=\"logistic\",solver=\"lbfgs\")\n",
    "clf2.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_mlp=clf2.predict(Validation_ngrams)\n",
    "f1_score(y_pred_mlp,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420163689695801"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "svc = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\"))\n",
    "bag = BaggingClassifier(svc,n_estimators=50, random_state=0,n_jobs=-1)\n",
    "bag.fit(Train_ngrams,df_training[\"label\"])\n",
    "y_pred_bag=bag.predict(Validation_ngrams)\n",
    "f1_score(y_pred_bag,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "\n",
    "for i in range(len(y_pred_log)):\n",
    "    if(y_pred_svm[i]==y_pred_mlp[i]):\n",
    "        y.append(y_pred_svm[i])\n",
    "    elif(y_pred_svm[i]==y_pred_rf[i]):\n",
    "        y.append(y_pred_svm[i])\n",
    "    elif(y_pred_rf[i]==y_pred_mlp[i]):\n",
    "        y.append(y_pred_log[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9354368180661479"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the shortened URL in its entirety: https://t.co/swClEhi2Iq\n",
      "\n",
      "The shortened URL forwards to:\n",
      "\n",
      "    https://www.thedailymash.co.uk/news/health/kids-reach-fk-this-shit-stage-of-lockdown-20200612197394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "shorturl = input(\"Enter the shortened URL in its entirety: \")\n",
    "r = requests.get(shorturl)\n",
    "\n",
    "print(\"\"\"\n",
    "The shortened URL forwards to:\n",
    "\n",
    "    %s\n",
    "\"\"\" % r.url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_training.tweet:\n",
    "    re.match(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred=clf.predict(Validation_ngrams)\n",
    "# f1_score(df_validation[\"label\"],y_pred,average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_test=clf.predict(Test_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_test=pd.DataFrame(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans=pd.DataFrame(df_test[\"id\"])\n",
    "# ans[\"label\"]=y_pred_test\n",
    "# ans.to_csv(\"answer9349valid.txt\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count=0\n",
    "# string=\"\"\n",
    "# for i in range(len(ans_sudhir.label)):\n",
    "#     if not((ans_sudhir.label[i]==ans.label[i]) and (ans.label[i] ==ans_MLP.label[i])):\n",
    "#         print(\"*************\")\n",
    "#         print(ans.id[i],ans_sudhir.label[i],ans.label[i],ans_MLP.label[i])\n",
    "#         print(df_test.tweet_original[i])\n",
    "#         print(\"*************\")\n",
    "# #         string+=\"\\n\"+\"\\n\" \"*************\"+\"\\n \"+str(ans.id[i])+\" Sudhir:\"+str(ans_sudhir.label[i])+\" Navneet:\"+ans.label[i]+\"\\n\"+df_test.tweet_original[i]\n",
    "#         count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file=open(\"mismatch.txt\",\"w\")\n",
    "# file.write(string)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9316940249105621"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_MLP=clf2.predict(Test_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_MLP=pd.DataFrame(df_test[\"id\"])\n",
    "ans_MLP[\"label\"]=y_pred_test_MLP\n",
    "# ans.to_csv(\"answer9349valid.txt\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>2136</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>2137</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>2138</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>2139</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>2140</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2140 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id label\n",
       "0        1  real\n",
       "1        2  fake\n",
       "2        3  fake\n",
       "3        4  real\n",
       "4        5  fake\n",
       "...    ...   ...\n",
       "2135  2136  real\n",
       "2136  2137  fake\n",
       "2137  2138  real\n",
       "2138  2139  real\n",
       "2139  2140  real\n",
       "\n",
       "[2140 rows x 2 columns]"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jun 23 13:54:19 2020\n",
    "\n",
    "@author: suranjana\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pysbd.utils import PySBDFactory\n",
    "import urllib\n",
    "import requests, re, spacy\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import csv\n",
    "import preprocessor as p\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "ua = UserAgent()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# f = open('CommonWords.txt')\n",
    "# commonEngWords = f.read().splitlines()\n",
    "# f.close()\n",
    "popular_links = [\n",
    "        \"nytimes\", \"wsj\", \"huffpost\", \"washingtonpost\",\"time\",\"republicworld\",\n",
    "        \"latimes\", \"reuters\", \"abcnews\", \"usatoday\",\n",
    "        \"bloomberg\", \"nbcnews\", \"dailymail\", \"theguardian\",\n",
    "        \"thesun\", \"mirror\", \"telegraph\", \"bbc\",\n",
    "        \"thestar\", \"theglobeandmail\", \"forbes\",\n",
    "        \"cnbc\", \"chinadaily\", \"nypost\", \"usnews\",\n",
    "        \"timesofindia\", \"thehindu\", \"hindustantimes\",\n",
    "        \"cbsnews\", \"sfgate\", \"thehill\", \"thedailybeast\",\n",
    "        \"newsweek\", \"theatlantic\", \"nzherald\", \"vanguardngr\",\n",
    "        \"dailysun\", \"thejakartapost\", \"thestar\",\n",
    "        \"straitstimes\", \"bangkokpost\", \"japantimes\",\n",
    "        \"thedailystar\", \"scmp\", \"yahoo.com/news\", \"news.google\"\n",
    "        ]\n",
    "\n",
    "\n",
    "def levenshtein_ratio_and_distance(s, t, ratio_calc = False):\n",
    "    \"\"\" levenshtein_ratio_and_distance:\n",
    "        Calculates levenshtein distance between two strings.\n",
    "        If ratio_calc = True, the function computes the\n",
    "        levenshtein distance ratio of similarity between two strings\n",
    "        For all i and j, distance[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    distance = np.zeros((rows,cols),dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1,cols):\n",
    "            distance[i][0] = i\n",
    "            distance[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
    "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
    "                if ratio_calc == True:\n",
    "                    cost = 2\n",
    "                else:\n",
    "                    cost = 1\n",
    "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
    "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
    "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
    "    if ratio_calc == True:\n",
    "        # Computation of the Levenshtein Distance Ratio\n",
    "        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))\n",
    "        return Ratio\n",
    "    else:\n",
    "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
    "        # insertions and/or substitutions\n",
    "        # This is the minimum number of edits needed to convert string a to string b\n",
    "        return \"The strings are {} edits away\".format(distance[row][col])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_search_result(query, number_result = 20):\n",
    "    query = urllib.parse.quote_plus(query) # Format into URL encoding\n",
    "    \n",
    "    \n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(number_result)\n",
    "    response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "    \n",
    "    links = []\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        # Checks if each element is present, else, raise exception\n",
    "        try:\n",
    "            link = r.find('a', href = True)\n",
    "            title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "            description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "            \n",
    "            # Check to make sure everything is present before appending\n",
    "            if link != '' and title != '' and description != '': \n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "        # Next loop if one element is not present\n",
    "        except:\n",
    "            continue\n",
    "    return links, titles, descriptions\n",
    "\n",
    " \n",
    "def clean_links(links, titles, descriptions):\n",
    "    to_remove = []\n",
    "    clean_links = []\n",
    "    for i, l in enumerate(links):\n",
    "        clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
    "    \n",
    "        # Anything that doesn't fit the above pattern will be removed\n",
    "        if clean is None:\n",
    "            to_remove.append(i)\n",
    "            continue\n",
    "        clean_links.append(clean.group(1))\n",
    "    \n",
    "    # Remove the corresponding titles & descriptions\n",
    "    for x in to_remove:\n",
    "        del titles[x]\n",
    "        del descriptions[x]\n",
    "    return clean_links, titles, descriptions\n",
    "\n",
    "\n",
    "def filter_links(links, titles, descriptions):\n",
    "    to_remove = []\n",
    "    for i, l in enumerate(links):\n",
    "        if not any(a in l for a in popular_links):\n",
    "            to_remove.append(i)\n",
    "            continue\n",
    "    \n",
    "    # Remove the corresponding titles & descriptions\n",
    "    links = [l for i,l in enumerate(links) if i not in to_remove]\n",
    "    titles = [t for i,t in enumerate(titles) if i not in to_remove]\n",
    "    links = [l for i,d in enumerate(descriptions) if i not in to_remove]\n",
    "    return links, titles, descriptions\n",
    "    \n",
    "\n",
    "def valid_links(links):\n",
    "    no_link = 0\n",
    "    for link in links:\n",
    "        if any(implink in link for implink in popular_links):\n",
    "            no_link = no_link+1\n",
    "    return no_link/len(links)\n",
    "\n",
    "\n",
    "def split_sentence(text):\n",
    "    nlp = spacy.blank('en')\n",
    "    nlp.add_pipe(PySBDFactory(nlp))\n",
    "    doc = nlp(text)\n",
    "    return([sent.text for sent in doc.sents if sent.text.isspace()==False])\n",
    "\n",
    "\n",
    "def valid_description(query, descriptions):\n",
    "    temp_query = preprocessing(query)\n",
    "    sentences = split_sentence(temp_query)\n",
    "    score = []\n",
    "    for q in query:\n",
    "        temp = [levenshtein_ratio_and_distance(q, sent,ratio_calc = True) for sent in sentences]\n",
    "        score.append(max(temp))\n",
    "    return sum(score)/len(score)\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())    \n",
    "    text = p.clean(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = text.lower().replace('[^\\w\\s]',' ').replace('\\s\\s+', ' ').replace('@','').replace('#','. ').replace('&amp;', 'and')\n",
    "    return text\n",
    "\n",
    "\n",
    "def translate_text(text):\n",
    "    result = translator.translate('Mitä sinä teet')\n",
    "    return result.text\n",
    "\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     query = \"'trade war'\"\n",
    "#     threshold = 0.3\n",
    "    tweets = []\n",
    "    clean_tweets = []\n",
    "    labels = []\n",
    "    with open('en_dataset.csv', 'rt') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count > 0:\n",
    "                tweets.append(row[2])\n",
    "                clean_tweets.append(preprocessing(row[2]))\n",
    "                labels.append(row[3])\n",
    "            line_count += 1\n",
    "#     score = []\n",
    "#     for query in tweets:\n",
    "#         links, titles, descriptions = get_search_result(query)\n",
    "#         links, titles, descriptions = filter_links(links, titles, descriptions)\n",
    "#         #descriptions = [preprocessing(desc) for desc in descriptions]\n",
    "#         if len(titles)>0:\n",
    "#             titles = [preprocessing(title.lower()) for title in titles]\n",
    "#             desc_score = valid_description(query, titles)\n",
    "#             score.append(desc_score)\n",
    "#         else:\n",
    "#             score.append(0)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good boy'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"i am a good boy\"\n",
    "# p.set_options(p.OPT.URL, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())    \n",
    "text = p.clean(text)\n",
    "text = remove_stopwords(text)\n",
    "text = text.lower().replace('[^\\w\\s]',' ').replace('\\s\\s+', ' ').replace('@','').replace('#','. ').replace('&amp;', 'and')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 533/6324 [07:52<1:25:32,  1.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-962cc3a64ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_original\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlinks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_search_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlinks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#descriptions = [preprocessing(desc) for desc in descriptions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-0950060d36ef>\u001b[0m in \u001b[0;36mget_search_result\u001b[0;34m(query, number_result)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mgoogle_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.google.com/search?q=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"&num=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoogle_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             httplib_response = self._make_request(conn, method, url,\n\u001b[0m\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score=[]\n",
    "for query in tqdm(df_training.tweet_original[96:]):\n",
    "    links, titles, descriptions = get_search_result(query)\n",
    "    links, titles, descriptions = filter_links(links, titles, descriptions)\n",
    "    #descriptions = [preprocessing(desc) for desc in descriptions]\n",
    "\n",
    "    if len(titles)>0:\n",
    "        titles = [preprocessing(title.lower()) for title in titles]\n",
    "        desc_score = valid_description(query, titles)\n",
    "        score.append(desc_score)\n",
    "    else:\n",
    "        score.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-7ff4f5f4210d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5135\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'label'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0125479261066574\n",
      "1 0.016719866241070046\n",
      "2 0.015267175572519082\n",
      "3 0.010056925996204944\n",
      "4 0.009802458078320187\n",
      "5 0.010391634734039156\n",
      "6 0\n",
      "7 0.019974635383639836\n",
      "8 0.03090909090909085\n",
      "9 0\n",
      "10 0\n",
      "11 0.011405901314158228\n",
      "12 0\n",
      "13 0.011572742955721709\n",
      "14 0\n",
      "15 0.0060990099009901036\n",
      "16 0.012350119904076693\n",
      "17 0.007973311092577129\n",
      "18 0.012672176308539935\n",
      "19 0\n",
      "20 0\n",
      "21 0\n",
      "22 0.01397646069777217\n",
      "23 0.01258414617392501\n",
      "24 0.017663639210773564\n",
      "25 0.01753312945973494\n",
      "26 0\n",
      "27 0\n",
      "28 0.010941331824184051\n",
      "29 0.011924119241192426\n",
      "30 0.008901651112706379\n",
      "31 0.018849206349206324\n",
      "32 0.010223478037503184\n",
      "33 0.025717566016073442\n",
      "34 0\n",
      "35 0.016246684350132608\n",
      "36 0\n",
      "37 0.03361344537815128\n",
      "38 0.023953823953823936\n",
      "39 0.021733073279465005\n",
      "40 0\n",
      "41 0.01848739495798316\n",
      "42 0.012285927029039502\n",
      "43 0.01005544980551189\n",
      "44 0.01993769470404981\n",
      "45 0.015142337976983628\n",
      "46 0.018779342723004706\n",
      "47 0.01143168209036472\n",
      "48 0.014456316781898201\n",
      "49 0.013580194846273864\n",
      "50 0.00981402986214539\n",
      "51 0.00919305413687438\n",
      "52 0.028780743066457372\n",
      "53 0.01309935739001482\n",
      "54 0.02481572481572483\n",
      "55 0.015240833932422718\n",
      "56 0.01876955161626692\n",
      "57 0.017009106645593432\n",
      "58 0.011296800103640333\n",
      "59 0.03672580143168379\n",
      "60 0\n",
      "61 0.016241610738255044\n",
      "62 0.037066881547139385\n",
      "63 0.013215859030837036\n",
      "64 0.02277992277992275\n",
      "65 0.028846153846153848\n",
      "66 0.015007215007214962\n",
      "67 0.022130532633158274\n",
      "68 0.009154228855721414\n",
      "69 0.03511904761904761\n",
      "70 0.01661807580174926\n",
      "71 0.006607604282022871\n",
      "72 0.010173512676679333\n",
      "73 0.022209002577830674\n",
      "74 0.03325238184195778\n",
      "75 0.017444114226644207\n",
      "76 0\n",
      "77 0\n",
      "78 0.0168771777003484\n",
      "79 0.0186353631694791\n",
      "80 0.008504696623508511\n",
      "81 0\n",
      "82 0.013955984970477746\n",
      "83 0.027115474520804136\n",
      "84 0.014555468135326504\n",
      "85 0.014554411106135234\n",
      "86 0.010421702927746414\n",
      "87 0.007140166713066852\n",
      "88 0.010403417180825844\n",
      "89 0.006303915063039125\n",
      "90 0.052455357142857144\n",
      "91 0.007131925313743475\n",
      "92 0.028324154209284007\n",
      "93 0.022700119474312986\n",
      "94 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(score[0:95])):\n",
    "#     print(i,score[i],df_training.label[i],s[i])\n",
    "        print(i,score[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis=score2+score+score3\n",
    "# len(score3)+len(score2)+len(score)\n",
    "# len(lis)\n",
    "# len(score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file=open(\"score_train_95\",'wb')\n",
    "pickle.dump(score,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6420"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "file=open(\"score_train\",'rb')\n",
    "s=pickle.load(file)\n",
    "file.close()\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lis=[]\n",
    "for i in lis:\n",
    "        train_lis.append([i])\n",
    "valid_lis=[]\n",
    "for i in score:\n",
    "        valid_lis.append([i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "csr_TrainFeature_without_ngram=csr_matrix(Train_f)\n",
    "csr_ValidFeature_without_ngram=csr_matrix(Validation_f)\n",
    "TrainFeatureFinal=hstack([Train_ngrams,csr_TrainFeature_without_ngram])\n",
    "ValidFeatureFinal=hstack([Validation_ngrams,csr_ValidFeature_without_ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto',kernel=\"sigmoid\"))\n",
    "clf.fit(TrainFeatureFinal,df_training[\"label\"])\n",
    "y_pred=clf.predict(ValidFeatureFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9223158467450243"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred,df_validation[\"label\"],average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.9247663551401869\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.9205357142857142\n",
      "-----------------\n",
      "Recall: 0.9347234814143246\n",
      "-----------------\n",
      "F1 Score: 0.9275753486279802\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.9294117647058824\n",
      "-----------------\n",
      "Recall: 0.914175506268081\n",
      "-----------------\n",
      "F1 Score: 0.921730675741371\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.9246530121846757\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative                 948                  89\n",
      "Actual Positive                  72                1031\n"
     ]
    }
   ],
   "source": [
    "func_Eval(y_pred,df_validation[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
